{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import tqdm\n",
    "import os\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms as T\n",
    "from torchvision.models.alexnet import AlexNet\n",
    "from torchvision.models.resnet import resnet18, resnet34, resnet50, resnet101, resnet152\n",
    "from torch.utils.data import IterableDataset, DataLoader, Dataset\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, average_precision_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "def linear_combination(x, y, epsilon): \n",
    "    return epsilon*x + (1-epsilon)*y\n",
    "\n",
    "\n",
    "def reduce_loss(loss, reduction='mean'):\n",
    "    return loss.mean() if reduction=='mean' else loss.sum() if reduction=='sum' else loss\n",
    "\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, epsilon:float=0.1, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, preds, target):\n",
    "        n = preds.size()[-1]\n",
    "        log_preds = F.log_softmax(preds, dim=-1)\n",
    "        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)\n",
    "        nll = F.nll_loss(log_preds, target, reduction=self.reduction)\n",
    "        return linear_combination(loss/n, nll, self.epsilon)\n",
    "    \n",
    "    \n",
    "class BonzTrainDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super(BonzTrainDataset, self).__init__()\n",
    "        self.image_prefixes = data.image_prefixes.values\n",
    "        self.features = data.features.values\n",
    "        self.img_tensors = data.img_tensors.values\n",
    "        if 'labels' in data:\n",
    "            self.labels = data.labels.values\n",
    "            self.one_hot_labels = data.one_hot_labels.values\n",
    "        else:\n",
    "            self.labels=None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 3000\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ''' Full data\n",
    "        x1 = idx // len(self.image_prefixes)\n",
    "        x2 = idx % len(self.image_prefixes)\n",
    "        '''\n",
    "        \n",
    "        # Random select\n",
    "        x1 = random.randint(0, len(self.image_prefixes)-1)\n",
    "        x2 = random.randint(0, len(self.image_prefixes)-1)\n",
    "        \n",
    "        if idx%3 == 0:\n",
    "            while self.labels[x1] != self.labels[x2]:\n",
    "                x2 = random.randint(0, len(self.image_prefixes)-1)\n",
    "        else:\n",
    "            while self.labels[x1] == self.labels[x2]:\n",
    "                x2 = random.randint(0, len(self.image_prefixes)-1)\n",
    "        \n",
    "        outputs = (self.img_tensors[x1], \n",
    "                   self.features[x1], \n",
    "                   self.img_tensors[x2], \n",
    "                   self.features[x2],)\n",
    "        if self.labels[x1] == self.labels[x2]:\n",
    "            outputs += (torch.tensor([1]),)\n",
    "        else:\n",
    "            outputs += (torch.tensor([0]),)\n",
    "        return outputs\n",
    "    \n",
    "        \n",
    "class Bonz(nn.Module):\n",
    "    def __init__(self, hidden_dim=100, feature_selection=12):\n",
    "        super(Bonz, self).__init__()\n",
    "        self.resnet = resnet50(True)\n",
    "        self.BiLSTM = nn.LSTM(2048, hidden_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.bn = nn.BatchNorm1d(hidden_dim*2 + feature_selection)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim*2 + feature_selection, hidden_dim*2 + feature_selection),\n",
    "            nn.LeakyReLU(0.001, inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim*2 + feature_selection, 1)\n",
    "        )\n",
    "        self.classifier.apply(self._init_weights)\n",
    "        self.classifier.apply(self._xavier)\n",
    "        self.BiLSTM.apply(self._xavier)\n",
    "    \n",
    "    def forward(self, x1, x1_f, x2, x2_f):\n",
    "        ''' PROCESS X1'''\n",
    "        # Generate featuress from each images\n",
    "        x = []\n",
    "        for img in x1:\n",
    "            temp = self.do_resnet(img)\n",
    "            x.append(temp.unsqueeze(0))\n",
    "        x = torch.cat(x, 0)\n",
    "        \n",
    "        # LSTM step\n",
    "        x, _ = self.BiLSTM(x)\n",
    "        lstm_features = x[:,-1,:]\n",
    "        \n",
    "        # Concate features\n",
    "        x1 = torch.cat([lstm_features, x1_f], -1)\n",
    "        #x1 = self.bn(x1)\n",
    "        \n",
    "        ''' PROCESS X2'''\n",
    "        # Generate featuress from each images\n",
    "        x = []\n",
    "        for img in x2:\n",
    "            temp = self.do_resnet(img)\n",
    "            x.append(temp.unsqueeze(0))\n",
    "        x = torch.cat(x, 0)\n",
    "        \n",
    "        # LSTM step\n",
    "        x, _ = self.BiLSTM(x)\n",
    "        lstm_features = x[:,-1,:]\n",
    "        \n",
    "        # Concate features\n",
    "        x2 = torch.cat([lstm_features, x2_f], -1)\n",
    "        #x2 = self.bn(x2)\n",
    "        \n",
    "        ''' DIFFERENCE BETWEEN X1 and X2'''\n",
    "        dif = torch.abs(x1-x2)   \n",
    "        \n",
    "        predict = self.classifier(dif)\n",
    "\n",
    "        return (predict, dif,)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding, nn.Conv1d)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, (nn.Linear, nn.Conv1d)) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "            \n",
    "    def _xavier(self, module):\n",
    "        for name, param in module.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "            elif 'bias' in name:\n",
    "                param.data.zero_()\n",
    "    \n",
    "    def do_resnet(self, img):\n",
    "        x = self.resnet.conv1(img)\n",
    "        x = self.resnet.bn1(x)\n",
    "        x = self.resnet.relu(x)\n",
    "        x = self.resnet.maxpool(x)\n",
    "\n",
    "        x = self.resnet.layer1(x)\n",
    "        x = self.resnet.layer2(x)\n",
    "        x = self.resnet.layer3(x)\n",
    "        x = self.resnet.layer4(x)\n",
    "\n",
    "        x = self.resnet.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def freeze_resnet(self):\n",
    "        for w in self.resnet.parameters():\n",
    "            w.requires_grad = False\n",
    "            \n",
    "    def unfreeze_resnet(self):\n",
    "        for w in self.resnet.parameters():\n",
    "            w.requires_grad = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e4de6a1fb5452497b08bf5268d0fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=280.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16c20bc365944958e24866ded3824bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=140.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_data(path):\n",
    "    data = pd.read_csv(path)\n",
    "    resnet_idx = list(data.columns).index('data_AUTOGRAPHER_RESNET_mean_tench, Tinca tinca')\n",
    "    new_data = data.iloc[:,:resnet_idx]\n",
    "    new_data = new_data.drop([col_len for col_len in new_data.keys() if '_len' in col_len], 1) # Drop columns with _LEN\n",
    "    new_data['labels'] = [int(i[-2:])-1 for i in new_data.event_id.values]\n",
    "    new_data['image_prefixes'] = list(map(lambda x,y,z: str(x)+'_'+str(y)+'_'+str(z), \n",
    "                                      new_data.sub_id.values, \n",
    "                                      new_data.source.values, \n",
    "                                      new_data.event_id.values))\n",
    "    new_data['one_hot_labels'] = list(map(lambda x: nn.functional.one_hot(torch.tensor(x), 20).float(), new_data.labels.values))\n",
    "    new_data['features'] = [torch.tensor(i).float() for i in new_data.iloc[:, 3:-3].values]\n",
    "    new_data['img_tensors'] = get_img_tensors(new_data['image_prefixes'].values)\n",
    "    return new_data.iloc[:, -5:]\n",
    "\n",
    "\n",
    "def get_test_data(path):\n",
    "    data = pd.read_csv(path, index_col=0)\n",
    "    resnet_idx = list(data.columns).index('data_AUTOGRAPHER_RESNET_mean_tench, Tinca tinca')\n",
    "    new_data = data.iloc[:,:resnet_idx]\n",
    "    new_data = new_data.drop([col_len for col_len in new_data.keys() if '_len' in col_len], 1) # Drop columns with _LEN\n",
    "    new_data['image_prefixes'] = list(map(lambda x,y: str(x)+'_pred'+str(y), \n",
    "                                      new_data.sub_id.values, \n",
    "                                      new_data.event_id.values))\n",
    "    new_data['features'] = [torch.tensor(i).float() for i in new_data.iloc[:, 3:-1].values]\n",
    "    new_data['img_tensors'] = get_img_tensors(new_data['image_prefixes'].values)\n",
    "    return new_data.iloc[:, [0,1,2,-3,-2,-1]]\n",
    "\n",
    "\n",
    "def get_img_tensors(image_prefixes):\n",
    "    transform = T.Compose([\n",
    "        T.Resize(256),\n",
    "        T.CenterCrop(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    temp = []\n",
    "    \n",
    "    for img_prefix in tqdm.notebook.tqdm(image_prefixes):\n",
    "        img_paths = []\n",
    "        i = 0\n",
    "        img_name = img_prefix+'_'+str(i)+'.jpg'\n",
    "        while img_name in os.listdir('./OUTPUT_MERGED/AUTOGRAPHER/'):\n",
    "            img_paths.append('./OUTPUT_MERGED/AUTOGRAPHER/'+img_name)\n",
    "            i += 1\n",
    "            img_name = img_prefix+'_'+str(i)+'.jpg'\n",
    "\n",
    "        # Transform images to tensors\n",
    "        img_tensors = []\n",
    "        for path in img_paths:\n",
    "            img = Image.open(path)\n",
    "            img_tensors.append(transform(img))\n",
    "            \n",
    "        # padding img tensors\n",
    "        if len(img_tensors) < 16:\n",
    "            dump = torch.zeros((3,224,224)).float()\n",
    "            dump = [dump] * (16 - len(img_tensors))\n",
    "            img_tensors.extend(dump)\n",
    "            \n",
    "        temp.append(torch.stack(img_tensors,0))\n",
    "    \n",
    "    return temp\n",
    "\n",
    "def check_params(model):\n",
    "    model.freeze_resnet()\n",
    "    print(sum([i.numel() for i in model.parameters() if i.requires_grad]))\n",
    "    model.unfreeze_resnet()\n",
    "    print(sum([i.numel() for i in model.parameters() if i.requires_grad]))\n",
    "    \n",
    "\n",
    "        \n",
    "train_data = get_data('train_min_max.csv')\n",
    "#train_data = train_data.sort_values(by='image_prefixes', ignore_index=True)\n",
    "test_data = get_test_data('test_min_max.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2746637\n",
      "28303669\n"
     ]
    }
   ],
   "source": [
    "model = Bonz(hidden_dim=128, feature_selection=train_data.features[0].shape[0])\n",
    "check_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = BonzTrainDataset(train_data)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Best Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3c9cb62a4d4d3896ffb5a7d66102f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e42190573684b3cb35e44d8df16e90b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training: ', max=47.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=0, Acc=71.63, Loss=0.48, LR=2.00e-03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aabf708fac9b4acea8338881320ac5ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training: ', max=47.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=1, Acc=75.27, Loss=0.39, LR=4.00e-03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b382e9982c7a4e66bcb44f4d6c560fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training: ', max=47.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=2, Acc=73.57, Loss=0.39, LR=6.00e-03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ac635159ce44e2a4de2bceac93254e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training: ', max=47.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=3, Acc=75.20, Loss=0.60, LR=8.00e-03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a7de03e4bbd407491c2ffb5e49b0b78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training: ', max=47.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=4, Acc=74.83, Loss=0.48, LR=1.00e-02\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca501b785204f859853b04f06c1bd39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed5c8ef11e174845bfedc208834d2127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training: ', max=47.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=0, Acc=75.93, Loss=0.34, LR=2.00e-02\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26f96ab261543f3a3f454d0913de5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training: ', max=47.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=1, Acc=76.33, Loss=0.47, LR=4.00e-02\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60fa13b8cf141ff8db273c0cc91b08e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training: ', max=47.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=2, Acc=74.03, Loss=0.49, LR=6.00e-02\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62468ee631e74d0e9b197f55667efe31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training: ', max=47.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=3, Acc=75.27, Loss=0.43, LR=8.00e-02\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d46bde589f1f4adcb75b79fea908617e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training: ', max=47.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch=4, Acc=70.33, Loss=0.52, LR=1.00e-01\n",
      "\n",
      "0.0020000000000000018\t0.7163333333333334\t24.896021991968155\n",
      "0.003999999999999999\t0.7526666666666667\t22.936850368976593\n",
      "0.006000000000000001\t0.7356666666666667\t23.772990942001343\n",
      "0.007999999999999998\t0.752\t23.320587068796158\n",
      "0.01\t0.7483333333333333\t23.09900239109993\n",
      "0.020000000000000018\t0.7593333333333333\t22.365572661161423\n",
      "0.039999999999999994\t0.7633333333333333\t22.559796035289764\n",
      "0.06000000000000001\t0.7403333333333333\t24.269256711006165\n",
      "0.07999999999999999\t0.7526666666666667\t24.01339900493622\n",
      "0.1\t0.7033333333333334\t28.055756986141205\n"
     ]
    }
   ],
   "source": [
    "model.freeze_resnet()\n",
    "\n",
    "model.to(DEVICE)\n",
    "#torch.save(model.state_dict(), 'origin_sd.pt')\n",
    "origin_state_dict = torch.load('origin_sd.pt')\n",
    "model.train()\n",
    "\n",
    "start_lr = 1e-3\n",
    "lr_find_epochs = 5\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), start_lr)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "\n",
    "# Make lists to capture the logs\n",
    "lr_find_acc = []\n",
    "lr_find_loss = []\n",
    "lr_find_lr = []\n",
    "\n",
    "\n",
    "for _ in range(2):\n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                                  base_lr=0, \n",
    "                                                  max_lr=start_lr*10, \n",
    "                                                  step_size_up=lr_find_epochs,\n",
    "                                                  cycle_momentum=False)\n",
    "    scheduler.step()\n",
    "    \n",
    "    for i in tqdm.notebook.trange(lr_find_epochs):\n",
    "        # Load origin state dict\n",
    "        model.load_state_dict(origin_state_dict)\n",
    "\n",
    "        predicts = []\n",
    "        y_true = []\n",
    "        total_loss = 0\n",
    "\n",
    "        for x1, x1_f, x2, x2_f, label in tqdm.notebook.tqdm(dataloader, desc='Training: '):\n",
    "\n",
    "            x1 = [ts.to(DEVICE) for ts in x1]\n",
    "            x1_f = x1_f.to(DEVICE)\n",
    "            x2 = [ts.to(DEVICE) for ts in x2]\n",
    "            x2_f = x2_f.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predict = model(x1, x1_f, x2, x2_f)[0]\n",
    "            loss = criterion(predict, label.float())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            predict = predict.detach().cpu()\n",
    "            predict = torch.sigmoid(predict)\n",
    "            predicts.extend(predict.tolist())\n",
    "            y_true.extend(label.detach().cpu().tolist())\n",
    "\n",
    "        train_acc = accuracy_score(np.array(y_true), np.array(predicts)>0.5)\n",
    "        lr_step = optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
    "        print(f'epoch={i}, Acc={train_acc*100:.2f}, Loss={loss:.2f}, LR={lr_step:.2e}')\n",
    "\n",
    "        lr_find_lr.append(lr_step)\n",
    "        lr_find_acc.append(train_acc)\n",
    "        lr_find_loss.append(total_loss)\n",
    "\n",
    "        scheduler.step()\n",
    "    \n",
    "    start_lr *= 10\n",
    "        \n",
    "        \n",
    "for a, b, c in zip(lr_find_lr, lr_find_acc, lr_find_loss):\n",
    "    print(f'{a}\\t{b}\\t{c}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993fa9db6a1b470c82aeee074ab37f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, Acc=72.17, Loss=0.50\n",
      "BEST MODEL at Epoch = 0\n",
      "epoch=0, Acc=75.33, Loss=0.55\n",
      "BEST MODEL at Epoch = 1\n",
      "epoch=0, Acc=75.50, Loss=0.44\n",
      "BEST MODEL at Epoch = 2\n",
      "epoch=0, Acc=76.40, Loss=0.35\n",
      "BEST MODEL at Epoch = 3\n",
      "epoch=0, Acc=75.87, Loss=0.36\n",
      "epoch=0, Acc=75.00, Loss=0.41\n",
      "epoch=0, Acc=74.83, Loss=0.41\n",
      "epoch=0, Acc=75.67, Loss=0.34\n",
      "epoch=0, Acc=76.23, Loss=0.39\n",
      "epoch=0, Acc=76.10, Loss=0.47\n",
      "epoch=0, Acc=74.23, Loss=0.50\n",
      "epoch=0, Acc=74.43, Loss=0.44\n",
      "epoch=0, Acc=74.00, Loss=0.46\n",
      "epoch=0, Acc=74.27, Loss=0.46\n",
      "epoch=0, Acc=74.67, Loss=0.51\n",
      "epoch=0, Acc=75.10, Loss=0.41\n",
      "epoch=0, Acc=75.97, Loss=0.39\n",
      "epoch=0, Acc=75.07, Loss=0.35\n",
      "epoch=0, Acc=74.80, Loss=0.41\n",
      "epoch=0, Acc=74.53, Loss=0.50\n",
      "\n",
      "{'train_loss': [26.103297501802444, 23.226665496826172, 22.845528066158295, 22.22298690676689, 22.338309109210968, 22.902508944272995, 23.020601972937584, 22.90515437722206, 22.793964117765427, 23.096015751361847, 23.716843843460083, 23.15110620856285, 23.965454638004303, 23.386887043714523, 23.924207717180252, 23.187666803598404, 22.95521456003189, 23.12922242283821, 23.260627508163452, 23.324531465768814], 'train_acc': [0.7216666666666667, 0.7533333333333333, 0.755, 0.764, 0.7586666666666667, 0.75, 0.7483333333333333, 0.7566666666666667, 0.7623333333333333, 0.761, 0.7423333333333333, 0.7443333333333333, 0.74, 0.7426666666666667, 0.7466666666666667, 0.751, 0.7596666666666667, 0.7506666666666667, 0.748, 0.7453333333333333]}\n"
     ]
    }
   ],
   "source": [
    "model.freeze_resnet()\n",
    "\n",
    "model.to(DEVICE)\n",
    "#torch.save(model.state_dict(), 'origin_sd.pt')\n",
    "origin_state_dict = torch.load('origin_sd.pt')\n",
    "model.train()\n",
    "\n",
    "start_lr = 2e-2\n",
    "lr_find_epochs = 20\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), start_lr)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "# Make lists to capture the logs\n",
    "best_metrics = {'train_loss': 1e10, \n",
    "                'train_acc': 0}\n",
    "\n",
    "metrics = {'train_loss': [], \n",
    "           'train_acc': []}\n",
    "\n",
    "    \n",
    "for e in tqdm.notebook.trange(lr_find_epochs):\n",
    "    # Load origin state dict\n",
    "    model.load_state_dict(origin_state_dict)\n",
    "\n",
    "    predicts = []\n",
    "    y_true = []\n",
    "    total_loss = 0\n",
    "\n",
    "    for x1, x1_f, x2, x2_f, label in dataloader:\n",
    "\n",
    "        x1 = [ts.to(DEVICE) for ts in x1]\n",
    "        x1_f = x1_f.to(DEVICE)\n",
    "        x2 = [ts.to(DEVICE) for ts in x2]\n",
    "        x2_f = x2_f.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predict = model(x1, x1_f, x2, x2_f)[0]\n",
    "        loss = criterion(predict, label.float())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        predict = predict.detach().cpu()\n",
    "        predict = torch.sigmoid(predict)\n",
    "        predicts.extend(predict.tolist())\n",
    "        y_true.extend(label.detach().cpu().tolist())\n",
    "\n",
    "    train_acc = accuracy_score(np.array(y_true), np.array(predicts)>0.5)\n",
    "    print(f'epoch={e}, Acc={train_acc*100:.2f}, Loss={loss:.2f}')\n",
    "\n",
    "    metrics['train_acc'].append(train_acc)\n",
    "    metrics['train_loss'].append(total_loss)\n",
    "    \n",
    "    if total_loss < best_metrics['train_loss']:\n",
    "        best_metrics['train_acc'] = train_acc\n",
    "        best_metrics['train_loss'] = total_loss\n",
    "        \n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "        print(f'BEST MODEL at Epoch = {e}')\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7453333333333333"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.array(y_true).squeeze(), (np.array(predicts)>0.5).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.17710277e-01, 6.68731630e-01, 2.39934889e-03, 5.80272079e-01,\n",
       "       2.56267581e-02, 6.34546997e-03, 4.51944172e-01, 9.04706836e-01,\n",
       "       6.41793071e-04, 2.21925691e-01])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(predicts).squeeze()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a90927c71e84832986ea4fdf8f587d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=0, Train_loss=12.0, Val_loss=12.0, Train_acc=5.71, Val_acc=8.57, LR=2.50e-02, , Time=8.76\n",
      "BEST MODEL at Epoch = 0\n",
      "Epoch=1, Train_loss=12.0, Val_loss=12.0, Train_acc=5.71, Val_acc=6.43, LR=2.50e-02, , Time=8.68\n",
      "BEST MODEL at Epoch = 1\n",
      "Epoch=2, Train_loss=12.0, Val_loss=11.9, Train_acc=11.43, Val_acc=10.00, LR=2.50e-02, , Time=8.72\n",
      "BEST MODEL at Epoch = 2\n",
      "Epoch=3, Train_loss=11.9, Val_loss=11.9, Train_acc=10.71, Val_acc=7.86, LR=2.50e-02, , Time=8.73\n",
      "BEST MODEL at Epoch = 3\n",
      "Epoch=4, Train_loss=11.9, Val_loss=11.9, Train_acc=6.43, Val_acc=5.00, LR=2.50e-02, , Time=8.77\n",
      "BEST MODEL at Epoch = 4\n",
      "Epoch=5, Train_loss=11.9, Val_loss=11.9, Train_acc=6.43, Val_acc=6.43, LR=2.50e-02, , Time=9.06\n",
      "BEST MODEL at Epoch = 5\n",
      "Epoch=6, Train_loss=11.8, Val_loss=11.8, Train_acc=10.00, Val_acc=13.57, LR=2.50e-02, , Time=8.80\n",
      "BEST MODEL at Epoch = 6\n",
      "Epoch=7, Train_loss=11.8, Val_loss=11.8, Train_acc=13.57, Val_acc=17.14, LR=2.50e-02, , Time=8.83\n",
      "BEST MODEL at Epoch = 7\n",
      "Epoch=8, Train_loss=11.8, Val_loss=11.7, Train_acc=19.29, Val_acc=17.14, LR=2.50e-02, , Time=8.88\n",
      "BEST MODEL at Epoch = 8\n",
      "Epoch=9, Train_loss=11.7, Val_loss=11.7, Train_acc=18.57, Val_acc=19.29, LR=2.50e-02, , Time=8.97\n",
      "BEST MODEL at Epoch = 9\n",
      "Epoch=10, Train_loss=11.6, Val_loss=11.6, Train_acc=22.14, Val_acc=13.57, LR=2.50e-02, , Time=9.09\n",
      "BEST MODEL at Epoch = 10\n",
      "Epoch=11, Train_loss=11.5, Val_loss=11.5, Train_acc=22.86, Val_acc=19.29, LR=2.50e-02, , Time=9.05\n",
      "BEST MODEL at Epoch = 11\n",
      "Epoch=12, Train_loss=11.4, Val_loss=11.4, Train_acc=25.00, Val_acc=18.57, LR=2.50e-02, , Time=9.39\n",
      "BEST MODEL at Epoch = 12\n",
      "Epoch=13, Train_loss=11.3, Val_loss=11.3, Train_acc=20.71, Val_acc=16.43, LR=2.50e-02, , Time=8.94\n",
      "BEST MODEL at Epoch = 13\n",
      "Epoch=14, Train_loss=11.2, Val_loss=11.2, Train_acc=22.14, Val_acc=17.86, LR=2.50e-02, , Time=9.24\n",
      "BEST MODEL at Epoch = 14\n",
      "Epoch=15, Train_loss=11.0, Val_loss=11.0, Train_acc=21.43, Val_acc=15.00, LR=2.50e-02, , Time=9.14\n",
      "BEST MODEL at Epoch = 15\n",
      "Epoch=16, Train_loss=10.9, Val_loss=10.9, Train_acc=23.57, Val_acc=14.29, LR=2.50e-02, , Time=9.06\n",
      "BEST MODEL at Epoch = 16\n",
      "Epoch=17, Train_loss=10.7, Val_loss=10.7, Train_acc=26.43, Val_acc=24.29, LR=2.50e-02, , Time=9.29\n",
      "BEST MODEL at Epoch = 17\n",
      "Epoch=18, Train_loss=10.4, Val_loss=10.5, Train_acc=29.29, Val_acc=22.14, LR=2.50e-02, , Time=9.11\n",
      "BEST MODEL at Epoch = 18\n",
      "Epoch=19, Train_loss=10.3, Val_loss=10.4, Train_acc=32.86, Val_acc=21.43, LR=2.66e-02, , Time=9.21\n",
      "BEST MODEL at Epoch = 19\n",
      "Epoch=20, Train_loss=10.1, Val_loss=10.2, Train_acc=30.00, Val_acc=25.00, LR=2.82e-02, , Time=9.21\n",
      "BEST MODEL at Epoch = 20\n",
      "Epoch=21, Train_loss=10.0, Val_loss=10.0, Train_acc=34.29, Val_acc=27.86, LR=2.98e-02, , Time=9.02\n",
      "BEST MODEL at Epoch = 21\n",
      "Epoch=22, Train_loss=9.8, Val_loss=9.8, Train_acc=30.00, Val_acc=29.29, LR=3.14e-02, , Time=9.17\n",
      "BEST MODEL at Epoch = 22\n",
      "Epoch=23, Train_loss=9.6, Val_loss=9.6, Train_acc=32.14, Val_acc=31.43, LR=3.30e-02, , Time=9.11\n",
      "BEST MODEL at Epoch = 23\n",
      "Epoch=24, Train_loss=9.4, Val_loss=9.5, Train_acc=35.71, Val_acc=27.86, LR=3.46e-02, , Time=9.11\n",
      "BEST MODEL at Epoch = 24\n",
      "Epoch=25, Train_loss=9.2, Val_loss=9.3, Train_acc=37.86, Val_acc=30.71, LR=3.62e-02, , Time=9.29\n",
      "BEST MODEL at Epoch = 25\n",
      "Epoch=26, Train_loss=9.0, Val_loss=9.2, Train_acc=35.71, Val_acc=34.29, LR=3.78e-02, , Time=9.07\n",
      "BEST MODEL at Epoch = 26\n",
      "Epoch=27, Train_loss=8.8, Val_loss=9.1, Train_acc=39.29, Val_acc=35.00, LR=3.94e-02, , Time=9.12\n",
      "BEST MODEL at Epoch = 27\n",
      "Epoch=28, Train_loss=8.7, Val_loss=8.9, Train_acc=35.00, Val_acc=33.57, LR=4.10e-02, , Time=9.09\n",
      "BEST MODEL at Epoch = 28\n",
      "Epoch=29, Train_loss=8.5, Val_loss=8.7, Train_acc=36.43, Val_acc=35.71, LR=4.26e-02, , Time=9.11\n",
      "BEST MODEL at Epoch = 29\n",
      "Epoch=30, Train_loss=8.3, Val_loss=8.7, Train_acc=40.00, Val_acc=35.71, LR=4.42e-02, , Time=9.07\n",
      "BEST MODEL at Epoch = 30\n",
      "Epoch=31, Train_loss=8.2, Val_loss=8.5, Train_acc=40.00, Val_acc=40.71, LR=4.58e-02, , Time=9.11\n",
      "BEST MODEL at Epoch = 31\n",
      "Epoch=32, Train_loss=8.0, Val_loss=8.4, Train_acc=43.57, Val_acc=36.43, LR=4.74e-02, , Time=9.23\n",
      "BEST MODEL at Epoch = 32\n",
      "Epoch=33, Train_loss=7.7, Val_loss=8.3, Train_acc=45.71, Val_acc=39.29, LR=4.90e-02, , Time=9.03\n",
      "BEST MODEL at Epoch = 33\n",
      "Epoch=34, Train_loss=7.6, Val_loss=8.2, Train_acc=45.71, Val_acc=34.29, LR=5.06e-02, , Time=9.16\n",
      "BEST MODEL at Epoch = 34\n",
      "Epoch=35, Train_loss=7.5, Val_loss=8.1, Train_acc=42.14, Val_acc=39.29, LR=5.22e-02, , Time=9.21\n",
      "BEST MODEL at Epoch = 35\n",
      "Epoch=36, Train_loss=7.3, Val_loss=8.0, Train_acc=47.14, Val_acc=34.29, LR=5.38e-02, , Time=8.92\n",
      "BEST MODEL at Epoch = 36\n",
      "Epoch=37, Train_loss=7.3, Val_loss=7.9, Train_acc=45.71, Val_acc=34.29, LR=5.54e-02, , Time=9.10\n",
      "BEST MODEL at Epoch = 37\n",
      "Epoch=38, Train_loss=6.9, Val_loss=7.6, Train_acc=51.43, Val_acc=42.14, LR=5.70e-02, , Time=9.19\n",
      "BEST MODEL at Epoch = 38\n",
      "Epoch=39, Train_loss=6.9, Val_loss=7.9, Train_acc=55.00, Val_acc=30.00, LR=5.86e-02, , Time=9.19\n",
      "Epoch=40, Train_loss=6.7, Val_loss=7.7, Train_acc=45.71, Val_acc=35.71, LR=6.02e-02, , Time=9.17\n",
      "Epoch=41, Train_loss=6.7, Val_loss=7.7, Train_acc=52.14, Val_acc=32.86, LR=6.18e-02, , Time=9.11\n",
      "Epoch=42, Train_loss=6.3, Val_loss=7.2, Train_acc=50.00, Val_acc=43.57, LR=6.34e-02, , Time=8.98\n",
      "BEST MODEL at Epoch = 42\n",
      "Epoch=43, Train_loss=6.3, Val_loss=7.5, Train_acc=50.71, Val_acc=32.86, LR=6.50e-02, , Time=9.06\n",
      "Epoch=44, Train_loss=6.3, Val_loss=7.2, Train_acc=47.86, Val_acc=37.14, LR=6.34e-02, , Time=9.07\n",
      "BEST MODEL at Epoch = 44\n",
      "Epoch=45, Train_loss=6.1, Val_loss=7.6, Train_acc=50.71, Val_acc=32.86, LR=6.18e-02, , Time=9.04\n",
      "Epoch=46, Train_loss=5.7, Val_loss=7.1, Train_acc=55.00, Val_acc=42.86, LR=6.02e-02, , Time=9.01\n",
      "BEST MODEL at Epoch = 46\n",
      "Epoch=47, Train_loss=6.1, Val_loss=7.5, Train_acc=53.57, Val_acc=30.71, LR=5.86e-02, , Time=9.18\n",
      "Epoch=48, Train_loss=5.5, Val_loss=7.0, Train_acc=61.43, Val_acc=37.86, LR=5.70e-02, , Time=9.13\n",
      "BEST MODEL at Epoch = 48\n",
      "Epoch=49, Train_loss=5.5, Val_loss=6.7, Train_acc=65.00, Val_acc=42.86, LR=5.54e-02, , Time=9.08\n",
      "BEST MODEL at Epoch = 49\n",
      "Epoch=50, Train_loss=5.7, Val_loss=6.7, Train_acc=51.43, Val_acc=50.00, LR=5.38e-02, , Time=8.98\n",
      "BEST MODEL at Epoch = 50\n",
      "Epoch=51, Train_loss=5.4, Val_loss=6.9, Train_acc=57.14, Val_acc=39.29, LR=5.22e-02, , Time=9.11\n",
      "Epoch=52, Train_loss=4.8, Val_loss=6.9, Train_acc=60.71, Val_acc=37.86, LR=5.06e-02, , Time=9.13\n",
      "Epoch=53, Train_loss=4.9, Val_loss=6.7, Train_acc=60.71, Val_acc=39.29, LR=4.90e-02, , Time=9.06\n",
      "BEST MODEL at Epoch = 53\n",
      "Epoch=54, Train_loss=5.3, Val_loss=7.3, Train_acc=62.14, Val_acc=34.29, LR=4.74e-02, , Time=9.07\n",
      "Epoch=55, Train_loss=4.8, Val_loss=6.7, Train_acc=60.00, Val_acc=40.71, LR=4.58e-02, , Time=9.03\n",
      "BEST MODEL at Epoch = 55\n",
      "Epoch=56, Train_loss=4.8, Val_loss=6.3, Train_acc=68.57, Val_acc=48.57, LR=4.42e-02, , Time=9.10\n",
      "BEST MODEL at Epoch = 56\n",
      "Epoch=57, Train_loss=4.8, Val_loss=6.2, Train_acc=61.43, Val_acc=49.29, LR=4.26e-02, , Time=9.04\n",
      "BEST MODEL at Epoch = 57\n",
      "Epoch=58, Train_loss=4.5, Val_loss=6.8, Train_acc=64.29, Val_acc=35.71, LR=4.10e-02, , Time=9.32\n",
      "Epoch=59, Train_loss=4.4, Val_loss=6.8, Train_acc=69.29, Val_acc=41.43, LR=3.94e-02, , Time=8.90\n",
      "Epoch=60, Train_loss=4.1, Val_loss=7.9, Train_acc=71.43, Val_acc=32.86, LR=3.78e-02, , Time=8.97\n",
      "Epoch=61, Train_loss=4.1, Val_loss=6.4, Train_acc=73.57, Val_acc=43.57, LR=3.62e-02, , Time=9.06\n",
      "Epoch=62, Train_loss=4.1, Val_loss=6.9, Train_acc=68.57, Val_acc=37.86, LR=3.46e-02, , Time=9.14\n",
      "Epoch=63, Train_loss=3.9, Val_loss=6.7, Train_acc=68.57, Val_acc=38.57, LR=3.30e-02, , Time=9.24\n",
      "Epoch=64, Train_loss=3.7, Val_loss=7.0, Train_acc=70.71, Val_acc=37.14, LR=3.14e-02, , Time=8.98\n",
      "Epoch=65, Train_loss=3.7, Val_loss=7.8, Train_acc=72.14, Val_acc=37.14, LR=2.98e-02, , Time=9.05\n",
      "Epoch=66, Train_loss=4.9, Val_loss=6.1, Train_acc=57.86, Val_acc=45.71, LR=2.82e-02, , Time=9.24\n",
      "BEST MODEL at Epoch = 66\n",
      "Epoch=67, Train_loss=4.7, Val_loss=6.0, Train_acc=64.29, Val_acc=50.00, LR=2.66e-02, , Time=9.05\n",
      "BEST MODEL at Epoch = 67\n",
      "Epoch=68, Train_loss=4.2, Val_loss=6.2, Train_acc=69.29, Val_acc=47.86, LR=2.50e-02, , Time=9.09\n",
      "Epoch=69, Train_loss=4.4, Val_loss=6.0, Train_acc=66.43, Val_acc=50.00, LR=2.66e-02, , Time=9.08\n",
      "BEST MODEL at Epoch = 69\n",
      "Epoch=70, Train_loss=3.8, Val_loss=6.0, Train_acc=77.86, Val_acc=47.86, LR=2.82e-02, , Time=9.02\n",
      "BEST MODEL at Epoch = 70\n",
      "Epoch=71, Train_loss=3.8, Val_loss=5.9, Train_acc=72.14, Val_acc=48.57, LR=2.98e-02, , Time=8.99\n",
      "BEST MODEL at Epoch = 71\n",
      "Epoch=72, Train_loss=3.6, Val_loss=6.1, Train_acc=77.14, Val_acc=49.29, LR=3.14e-02, , Time=9.10\n",
      "Epoch=73, Train_loss=3.5, Val_loss=6.4, Train_acc=77.14, Val_acc=43.57, LR=3.30e-02, , Time=9.18\n",
      "Epoch=74, Train_loss=3.5, Val_loss=6.3, Train_acc=76.43, Val_acc=40.71, LR=3.46e-02, , Time=9.11\n",
      "Epoch=75, Train_loss=3.2, Val_loss=6.7, Train_acc=73.57, Val_acc=39.29, LR=3.62e-02, , Time=8.94\n",
      "Epoch=76, Train_loss=3.8, Val_loss=7.4, Train_acc=67.86, Val_acc=36.43, LR=3.78e-02, , Time=9.01\n",
      "Epoch=77, Train_loss=4.1, Val_loss=6.4, Train_acc=68.57, Val_acc=45.71, LR=3.94e-02, , Time=9.14\n",
      "Epoch=78, Train_loss=3.2, Val_loss=6.8, Train_acc=76.43, Val_acc=40.71, LR=4.10e-02, , Time=9.21\n",
      "Epoch=79, Train_loss=3.1, Val_loss=6.8, Train_acc=75.71, Val_acc=40.00, LR=4.26e-02, , Time=9.02\n",
      "Epoch=80, Train_loss=3.1, Val_loss=6.6, Train_acc=76.43, Val_acc=42.14, LR=4.42e-02, , Time=9.03\n",
      "Epoch=81, Train_loss=3.1, Val_loss=6.5, Train_acc=76.43, Val_acc=40.00, LR=4.58e-02, , Time=9.02\n",
      "Epoch=82, Train_loss=3.2, Val_loss=7.2, Train_acc=80.00, Val_acc=40.00, LR=4.74e-02, , Time=9.05\n",
      "Epoch=83, Train_loss=3.2, Val_loss=8.1, Train_acc=75.00, Val_acc=33.57, LR=4.90e-02, , Time=9.19\n",
      "Epoch=84, Train_loss=2.8, Val_loss=6.6, Train_acc=81.43, Val_acc=42.14, LR=5.06e-02, , Time=8.98\n",
      "Epoch=85, Train_loss=2.9, Val_loss=6.2, Train_acc=75.71, Val_acc=45.71, LR=5.22e-02, , Time=9.30\n",
      "Epoch=86, Train_loss=3.1, Val_loss=7.4, Train_acc=75.71, Val_acc=39.29, LR=5.38e-02, , Time=9.23\n",
      "Epoch=87, Train_loss=2.8, Val_loss=6.8, Train_acc=80.71, Val_acc=43.57, LR=5.54e-02, , Time=8.97\n",
      "Epoch=88, Train_loss=2.9, Val_loss=6.4, Train_acc=75.00, Val_acc=48.57, LR=5.70e-02, , Time=8.95\n",
      "Epoch=89, Train_loss=2.7, Val_loss=6.5, Train_acc=79.29, Val_acc=47.86, LR=5.86e-02, , Time=9.03\n",
      "Epoch=90, Train_loss=2.5, Val_loss=7.5, Train_acc=82.86, Val_acc=41.43, LR=6.02e-02, , Time=9.05\n",
      "Epoch=91, Train_loss=3.0, Val_loss=6.4, Train_acc=77.14, Val_acc=45.00, LR=6.18e-02, , Time=9.16\n",
      "Epoch=92, Train_loss=2.5, Val_loss=6.6, Train_acc=78.57, Val_acc=46.43, LR=6.34e-02, , Time=9.05\n",
      "Epoch=93, Train_loss=3.3, Val_loss=6.8, Train_acc=75.00, Val_acc=46.43, LR=6.50e-02, , Time=9.14\n",
      "Epoch=94, Train_loss=2.8, Val_loss=6.5, Train_acc=77.14, Val_acc=48.57, LR=6.34e-02, , Time=9.22\n",
      "Epoch=95, Train_loss=2.8, Val_loss=5.7, Train_acc=82.14, Val_acc=55.00, LR=6.18e-02, , Time=9.10\n",
      "BEST MODEL at Epoch = 95\n",
      "Epoch=96, Train_loss=2.7, Val_loss=6.3, Train_acc=77.14, Val_acc=47.14, LR=6.02e-02, , Time=8.88\n",
      "Epoch=97, Train_loss=2.4, Val_loss=6.7, Train_acc=85.00, Val_acc=47.14, LR=5.86e-02, , Time=9.02\n",
      "Epoch=98, Train_loss=2.7, Val_loss=6.7, Train_acc=78.57, Val_acc=43.57, LR=5.70e-02, , Time=9.08\n",
      "Epoch=99, Train_loss=2.7, Val_loss=7.6, Train_acc=77.14, Val_acc=42.86, LR=5.54e-02, , Time=9.11\n",
      "Epoch=100, Train_loss=2.3, Val_loss=6.6, Train_acc=84.29, Val_acc=47.86, LR=5.38e-02, , Time=8.98\n",
      "Epoch=101, Train_loss=2.4, Val_loss=6.5, Train_acc=84.29, Val_acc=47.86, LR=5.22e-02, , Time=9.08\n",
      "Epoch=102, Train_loss=2.1, Val_loss=7.0, Train_acc=86.43, Val_acc=44.29, LR=5.06e-02, , Time=9.17\n",
      "Epoch=103, Train_loss=2.3, Val_loss=5.9, Train_acc=82.14, Val_acc=55.71, LR=4.90e-02, , Time=9.06\n",
      "Epoch=104, Train_loss=2.1, Val_loss=7.0, Train_acc=86.43, Val_acc=44.29, LR=4.74e-02, , Time=9.13\n",
      "Epoch=105, Train_loss=2.1, Val_loss=6.8, Train_acc=89.29, Val_acc=47.86, LR=4.58e-02, , Time=9.03\n",
      "Epoch=106, Train_loss=2.3, Val_loss=6.6, Train_acc=84.29, Val_acc=49.29, LR=4.42e-02, , Time=9.22\n",
      "Epoch=107, Train_loss=1.9, Val_loss=6.6, Train_acc=87.14, Val_acc=48.57, LR=4.26e-02, , Time=9.07\n",
      "Epoch=108, Train_loss=1.9, Val_loss=7.3, Train_acc=87.14, Val_acc=44.29, LR=4.10e-02, , Time=8.98\n",
      "Epoch=109, Train_loss=2.2, Val_loss=7.4, Train_acc=80.00, Val_acc=44.29, LR=3.94e-02, , Time=9.03\n",
      "Epoch=110, Train_loss=2.2, Val_loss=9.1, Train_acc=84.29, Val_acc=44.29, LR=3.78e-02, , Time=9.01\n",
      "Epoch=111, Train_loss=2.1, Val_loss=8.9, Train_acc=87.14, Val_acc=37.14, LR=3.62e-02, , Time=9.13\n",
      "Epoch=112, Train_loss=1.8, Val_loss=7.3, Train_acc=90.00, Val_acc=42.14, LR=3.46e-02, , Time=9.10\n",
      "Epoch=113, Train_loss=1.7, Val_loss=7.0, Train_acc=87.86, Val_acc=45.00, LR=3.30e-02, , Time=9.09\n",
      "Epoch=114, Train_loss=1.7, Val_loss=7.0, Train_acc=87.14, Val_acc=43.57, LR=3.14e-02, , Time=9.00\n",
      "Epoch=115, Train_loss=1.7, Val_loss=7.0, Train_acc=91.43, Val_acc=44.29, LR=2.98e-02, , Time=9.11\n",
      "Epoch=116, Train_loss=1.7, Val_loss=7.2, Train_acc=87.86, Val_acc=41.43, LR=2.82e-02, , Time=9.05\n",
      "Epoch=117, Train_loss=1.5, Val_loss=7.1, Train_acc=92.86, Val_acc=44.29, LR=2.66e-02, , Time=9.17\n",
      "Epoch=118, Train_loss=1.7, Val_loss=7.5, Train_acc=87.86, Val_acc=40.71, LR=2.50e-02, , Time=9.09\n",
      "Epoch=119, Train_loss=1.5, Val_loss=7.7, Train_acc=90.00, Val_acc=41.43, LR=2.66e-02, , Time=8.97\n",
      "Epoch=120, Train_loss=1.6, Val_loss=7.4, Train_acc=88.57, Val_acc=45.00, LR=2.82e-02, , Time=9.11\n",
      "Epoch=121, Train_loss=1.5, Val_loss=7.5, Train_acc=91.43, Val_acc=45.00, LR=2.98e-02, , Time=9.23\n",
      "Epoch=122, Train_loss=1.5, Val_loss=7.2, Train_acc=92.14, Val_acc=43.57, LR=3.14e-02, , Time=9.00\n",
      "Epoch=123, Train_loss=1.4, Val_loss=7.2, Train_acc=92.86, Val_acc=43.57, LR=3.30e-02, , Time=9.01\n",
      "Epoch=124, Train_loss=1.4, Val_loss=7.8, Train_acc=94.29, Val_acc=45.00, LR=3.46e-02, , Time=9.11\n",
      "Epoch=125, Train_loss=1.5, Val_loss=7.5, Train_acc=95.00, Val_acc=45.71, LR=3.62e-02, , Time=9.09\n",
      "Epoch=126, Train_loss=1.3, Val_loss=7.4, Train_acc=95.00, Val_acc=46.43, LR=3.78e-02, , Time=9.15\n",
      "Epoch=127, Train_loss=1.3, Val_loss=7.5, Train_acc=95.00, Val_acc=45.00, LR=3.94e-02, , Time=9.01\n",
      "Epoch=128, Train_loss=1.3, Val_loss=7.6, Train_acc=95.00, Val_acc=43.57, LR=4.10e-02, , Time=9.06\n",
      "Epoch=129, Train_loss=1.4, Val_loss=8.5, Train_acc=92.14, Val_acc=39.29, LR=4.26e-02, , Time=9.06\n",
      "Epoch=130, Train_loss=1.3, Val_loss=8.2, Train_acc=92.86, Val_acc=39.29, LR=4.42e-02, , Time=9.04\n",
      "Epoch=131, Train_loss=1.5, Val_loss=7.4, Train_acc=91.43, Val_acc=42.14, LR=4.58e-02, , Time=9.26\n",
      "Epoch=132, Train_loss=1.4, Val_loss=8.5, Train_acc=91.43, Val_acc=40.71, LR=4.74e-02, , Time=8.95\n",
      "Epoch=133, Train_loss=1.5, Val_loss=9.0, Train_acc=91.43, Val_acc=39.29, LR=4.90e-02, , Time=9.19\n",
      "Epoch=134, Train_loss=2.4, Val_loss=6.4, Train_acc=81.43, Val_acc=53.57, LR=5.06e-02, , Time=9.09\n",
      "Epoch=135, Train_loss=2.2, Val_loss=6.9, Train_acc=82.14, Val_acc=46.43, LR=5.22e-02, , Time=9.14\n",
      "Epoch=136, Train_loss=2.3, Val_loss=6.1, Train_acc=82.86, Val_acc=57.14, LR=5.38e-02, , Time=9.04\n",
      "Epoch=137, Train_loss=2.6, Val_loss=6.7, Train_acc=79.29, Val_acc=50.71, LR=5.54e-02, , Time=9.06\n",
      "Epoch=138, Train_loss=2.2, Val_loss=6.2, Train_acc=83.57, Val_acc=57.14, LR=5.70e-02, , Time=8.98\n",
      "Epoch=139, Train_loss=2.1, Val_loss=6.2, Train_acc=84.29, Val_acc=55.00, LR=5.86e-02, , Time=9.37\n",
      "Epoch=140, Train_loss=2.2, Val_loss=6.0, Train_acc=81.43, Val_acc=55.71, LR=6.02e-02, , Time=9.09\n",
      "Epoch=141, Train_loss=1.8, Val_loss=6.3, Train_acc=86.43, Val_acc=57.14, LR=6.18e-02, , Time=8.98\n",
      "Epoch=142, Train_loss=1.8, Val_loss=6.0, Train_acc=87.86, Val_acc=53.57, LR=6.34e-02, , Time=9.23\n",
      "Epoch=143, Train_loss=1.8, Val_loss=6.2, Train_acc=92.14, Val_acc=56.43, LR=6.50e-02, , Time=9.10\n",
      "Epoch=144, Train_loss=1.7, Val_loss=6.2, Train_acc=91.43, Val_acc=55.00, LR=6.34e-02, , Time=9.02\n",
      "Epoch=145, Train_loss=1.6, Val_loss=6.2, Train_acc=91.43, Val_acc=55.00, LR=6.18e-02, , Time=9.21\n",
      "Epoch=146, Train_loss=1.7, Val_loss=6.1, Train_acc=91.43, Val_acc=52.86, LR=6.02e-02, , Time=8.94\n",
      "Epoch=147, Train_loss=1.8, Val_loss=6.1, Train_acc=91.43, Val_acc=57.14, LR=5.86e-02, , Time=9.10\n",
      "Epoch=148, Train_loss=1.5, Val_loss=6.3, Train_acc=93.57, Val_acc=56.43, LR=5.70e-02, , Time=9.07\n",
      "Epoch=149, Train_loss=1.5, Val_loss=6.2, Train_acc=94.29, Val_acc=55.00, LR=5.54e-02, , Time=9.30\n",
      "Epoch=150, Train_loss=1.4, Val_loss=6.2, Train_acc=92.14, Val_acc=55.00, LR=5.38e-02, , Time=9.24\n",
      "Epoch=151, Train_loss=1.2, Val_loss=6.3, Train_acc=97.14, Val_acc=55.00, LR=5.22e-02, , Time=8.95\n",
      "Epoch=152, Train_loss=1.3, Val_loss=6.2, Train_acc=94.29, Val_acc=58.57, LR=5.06e-02, , Time=9.02\n",
      "Epoch=153, Train_loss=1.3, Val_loss=6.2, Train_acc=94.29, Val_acc=57.14, LR=4.90e-02, , Time=9.15\n",
      "Epoch=154, Train_loss=1.5, Val_loss=6.3, Train_acc=95.00, Val_acc=57.14, LR=4.74e-02, , Time=9.07\n",
      "Epoch=155, Train_loss=1.4, Val_loss=6.4, Train_acc=92.14, Val_acc=54.29, LR=4.58e-02, , Time=9.19\n",
      "Epoch=156, Train_loss=1.3, Val_loss=6.4, Train_acc=93.57, Val_acc=55.00, LR=4.42e-02, , Time=9.09\n",
      "Epoch=157, Train_loss=1.2, Val_loss=6.3, Train_acc=95.71, Val_acc=55.71, LR=4.26e-02, , Time=9.10\n",
      "Epoch=158, Train_loss=1.1, Val_loss=6.4, Train_acc=96.43, Val_acc=54.29, LR=4.10e-02, , Time=9.07\n",
      "Epoch=159, Train_loss=1.0, Val_loss=6.4, Train_acc=95.71, Val_acc=56.43, LR=3.94e-02, , Time=9.13\n",
      "Epoch=160, Train_loss=1.0, Val_loss=6.5, Train_acc=96.43, Val_acc=55.00, LR=3.78e-02, , Time=9.20\n",
      "Epoch=161, Train_loss=1.0, Val_loss=6.8, Train_acc=97.14, Val_acc=51.43, LR=3.62e-02, , Time=8.91\n",
      "Epoch=162, Train_loss=0.9, Val_loss=6.7, Train_acc=95.71, Val_acc=52.14, LR=3.46e-02, , Time=9.19\n",
      "Epoch=163, Train_loss=0.9, Val_loss=6.9, Train_acc=98.57, Val_acc=51.43, LR=3.30e-02, , Time=9.04\n",
      "Epoch=164, Train_loss=0.8, Val_loss=6.9, Train_acc=97.86, Val_acc=51.43, LR=3.14e-02, , Time=9.11\n",
      "Epoch=165, Train_loss=1.0, Val_loss=7.2, Train_acc=97.14, Val_acc=47.14, LR=2.98e-02, , Time=9.09\n",
      "Epoch=166, Train_loss=0.9, Val_loss=7.1, Train_acc=96.43, Val_acc=55.00, LR=2.82e-02, , Time=9.05\n",
      "Epoch=167, Train_loss=0.8, Val_loss=6.7, Train_acc=98.57, Val_acc=55.00, LR=2.66e-02, , Time=9.13\n",
      "Epoch=168, Train_loss=0.8, Val_loss=6.4, Train_acc=99.29, Val_acc=55.71, LR=2.50e-02, , Time=9.09\n",
      "Epoch=169, Train_loss=0.9, Val_loss=6.9, Train_acc=98.57, Val_acc=52.86, LR=2.66e-02, , Time=9.17\n",
      "Epoch=170, Train_loss=0.8, Val_loss=7.4, Train_acc=96.43, Val_acc=52.14, LR=2.82e-02, , Time=9.08\n",
      "Epoch=171, Train_loss=0.8, Val_loss=7.4, Train_acc=97.14, Val_acc=50.71, LR=2.98e-02, , Time=9.00\n",
      "Epoch=172, Train_loss=0.8, Val_loss=7.9, Train_acc=96.43, Val_acc=48.57, LR=3.14e-02, , Time=9.10\n",
      "Epoch=173, Train_loss=0.9, Val_loss=8.0, Train_acc=97.86, Val_acc=45.71, LR=3.30e-02, , Time=9.18\n",
      "Epoch=174, Train_loss=0.7, Val_loss=7.1, Train_acc=99.29, Val_acc=50.71, LR=3.46e-02, , Time=9.03\n",
      "Epoch=175, Train_loss=0.8, Val_loss=6.9, Train_acc=97.86, Val_acc=52.86, LR=3.62e-02, , Time=9.08\n",
      "Epoch=176, Train_loss=1.0, Val_loss=8.7, Train_acc=95.71, Val_acc=45.71, LR=3.78e-02, , Time=9.05\n",
      "Epoch=177, Train_loss=1.0, Val_loss=6.9, Train_acc=97.14, Val_acc=52.14, LR=3.94e-02, , Time=9.29\n",
      "Epoch=178, Train_loss=0.8, Val_loss=7.7, Train_acc=97.86, Val_acc=49.29, LR=4.10e-02, , Time=8.94\n",
      "Epoch=179, Train_loss=0.7, Val_loss=8.4, Train_acc=97.86, Val_acc=45.71, LR=4.26e-02, , Time=9.11\n",
      "Epoch=180, Train_loss=0.8, Val_loss=8.1, Train_acc=97.86, Val_acc=50.00, LR=4.42e-02, , Time=9.20\n",
      "Epoch=181, Train_loss=0.7, Val_loss=8.3, Train_acc=100.00, Val_acc=47.14, LR=4.58e-02, , Time=9.15\n",
      "Epoch=182, Train_loss=0.7, Val_loss=8.2, Train_acc=100.00, Val_acc=48.57, LR=4.74e-02, , Time=9.15\n",
      "Epoch=183, Train_loss=0.7, Val_loss=8.0, Train_acc=98.57, Val_acc=47.86, LR=4.90e-02, , Time=9.01\n",
      "Epoch=184, Train_loss=0.7, Val_loss=8.5, Train_acc=98.57, Val_acc=49.29, LR=5.06e-02, , Time=9.20\n",
      "Epoch=185, Train_loss=0.6, Val_loss=8.1, Train_acc=99.29, Val_acc=48.57, LR=5.22e-02, , Time=9.12\n",
      "Epoch=186, Train_loss=0.6, Val_loss=9.0, Train_acc=96.43, Val_acc=45.00, LR=5.38e-02, , Time=9.24\n",
      "Epoch=187, Train_loss=0.6, Val_loss=8.4, Train_acc=99.29, Val_acc=48.57, LR=5.54e-02, , Time=8.94\n",
      "Epoch=188, Train_loss=0.6, Val_loss=8.9, Train_acc=100.00, Val_acc=45.00, LR=5.70e-02, , Time=9.03\n",
      "Epoch=189, Train_loss=0.6, Val_loss=8.5, Train_acc=98.57, Val_acc=50.00, LR=5.86e-02, , Time=9.07\n",
      "Epoch=190, Train_loss=0.5, Val_loss=9.5, Train_acc=99.29, Val_acc=42.86, LR=6.02e-02, , Time=9.28\n",
      "Epoch=191, Train_loss=0.7, Val_loss=9.9, Train_acc=97.86, Val_acc=42.14, LR=6.18e-02, , Time=9.15\n",
      "Epoch=192, Train_loss=0.6, Val_loss=8.8, Train_acc=99.29, Val_acc=49.29, LR=6.34e-02, , Time=9.04\n",
      "Epoch=193, Train_loss=0.6, Val_loss=9.3, Train_acc=98.57, Val_acc=44.29, LR=6.50e-02, , Time=9.08\n",
      "Epoch=194, Train_loss=0.5, Val_loss=8.9, Train_acc=99.29, Val_acc=49.29, LR=6.34e-02, , Time=9.22\n",
      "Epoch=195, Train_loss=0.5, Val_loss=9.4, Train_acc=99.29, Val_acc=45.00, LR=6.18e-02, , Time=9.13\n",
      "Epoch=196, Train_loss=0.8, Val_loss=7.4, Train_acc=97.86, Val_acc=52.14, LR=6.02e-02, , Time=9.18\n",
      "Epoch=197, Train_loss=0.7, Val_loss=9.6, Train_acc=96.43, Val_acc=42.86, LR=5.86e-02, , Time=8.95\n",
      "Epoch=198, Train_loss=0.6, Val_loss=8.0, Train_acc=97.86, Val_acc=49.29, LR=5.70e-02, , Time=9.00\n",
      "Epoch=199, Train_loss=0.5, Val_loss=7.9, Train_acc=99.29, Val_acc=47.86, LR=5.54e-02, , Time=9.23\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_LR = 0.065\n",
    "MIN_LR = 0.025\n",
    "NUM_EPOCH = 150\n",
    "\n",
    "\n",
    "# Split data\n",
    "train_dataset = train_data.loc[140:]\n",
    "val_dataset = train_data.loc[:139]\n",
    "\n",
    "train_dataset = BonzDataset(train_dataset)\n",
    "val_dataset = BonzDataset(val_dataset)\n",
    "\n",
    "data_loader = DataLoader(train_dataset, batch_size=40, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=40)\n",
    "\n",
    "best_metrics = {'train_loss': 1e10, \n",
    "                'val_loss': 1e10, \n",
    "                'train_acc': 0, \n",
    "                'val_acc': 0}\n",
    "\n",
    "''' TRAINING WITH FREEZE ResNet\n",
    "'''\n",
    "model.to(DEVICE)\n",
    "torch.save(model.state_dict(), 'origin_sd.pt')\n",
    "origin_state_dict = torch.load('origin_sd.pt')\n",
    "\n",
    "model.freeze_resnet()\n",
    "model.load_state_dict(origin_state_dict) #Load origin state dict\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, betas=(0.9, 0.99))\n",
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=MAX_LR)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=10, verbose=True)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                              base_lr=MIN_LR, \n",
    "                                              max_lr=MAX_LR, \n",
    "                                              step_size_up=(NUM_EPOCH//8)*len(data_loader))\n",
    "\n",
    "for e in tqdm.notebook.trange(NUM_EPOCH):\n",
    "    \n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    s = time.time()\n",
    "    train_loss = 0\n",
    "    predicts = []\n",
    "    y_true = []\n",
    "    \n",
    "    for img_tensors, features, one_hot_label, label in data_loader:\n",
    "        img_tensors = [ts.to(DEVICE) for ts in img_tensors]\n",
    "        features = features.to(DEVICE)\n",
    "        one_hot_label = one_hot_label.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predict = model(img_tensors, features)\n",
    "        loss = loss_fn(predict, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if e >= 19:\n",
    "            scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        predict = predict.detach().cpu()\n",
    "        predict = torch.argmax(predict, 1)\n",
    "        predicts.extend(predict.tolist())\n",
    "        y_true.extend(label.detach().cpu().tolist())\n",
    "\n",
    "    train_acc = accuracy_score(y_true, predicts)\n",
    "    lr_step = optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    predicts = []\n",
    "    y_true = []\n",
    "    \n",
    "    for img_tensors, features, one_hot_label, label in val_data_loader:\n",
    "        img_tensors = [ts.to(DEVICE) for ts in img_tensors]\n",
    "        features = features.to(DEVICE)\n",
    "        one_hot_label = one_hot_label.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predict = model(img_tensors, features)\n",
    "            loss = loss_fn(predict, label)\n",
    "            val_loss += loss.item()\n",
    "        predict = predict.detach().cpu()\n",
    "        predict = torch.argmax(predict, 1)\n",
    "        predicts.extend(predict.tolist())\n",
    "        y_true.extend(label.detach().cpu().tolist())\n",
    "\n",
    "    val_acc = accuracy_score(y_true, predicts)\n",
    "\n",
    "    # Scheduler step wise\n",
    "    #scheduler.step(0.25*train_loss + 0.75*val_loss)\n",
    "    \n",
    "    print(f'Epoch={e}, Train_loss={train_loss:.1f}, Val_loss={val_loss:.1f}, Train_acc={train_acc*100:.2f}, Val_acc={val_acc*100:.2f}, LR={lr_step:.2e}, , Time={time.time()-s:.2f}')\n",
    "    \n",
    "    if val_loss < best_metrics['val_loss']:\n",
    "        best_metrics['train_acc'] = train_acc\n",
    "        best_metrics['val_acc'] = val_acc\n",
    "        best_metrics['train_loss'] = train_loss\n",
    "        best_metrics['val_loss'] = val_loss\n",
    "\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "        print(f'BEST MODEL at Epoch = {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8264700613043315"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "#model.load_state_dict(torch.load('best_model.pt'))\n",
    "predict_matrix = []\n",
    "val_loss = 0\n",
    "val_data_loader = DataLoader(BonzDataset(train_data), batch_size=32)\n",
    "for img_tensors, features, one_hot_label, label in val_data_loader:\n",
    "    img_tensors = [ts.to(DEVICE) for ts in img_tensors]\n",
    "    features = features.to(DEVICE)\n",
    "    one_hot_label = one_hot_label.to(DEVICE)\n",
    "    label = label.to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predict = model(img_tensors, features)\n",
    "    predict = predict.detach().cpu()\n",
    "    predict_matrix.extend(torch.softmax(predict, 1))\n",
    "    \n",
    "onehot_true = np.array(list(i.numpy() for i in train_data.one_hot_labels.values))\n",
    "onehot_predict = torch.stack(predict_matrix).numpy()\n",
    "\n",
    "average_precision_score(onehot_true, onehot_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 11,  7,  3,  5,  5,  6,  7,  6,  9, 14, 11, 12, 13, 14,  1, 16,\n",
       "       17, 18, 19, 10,  1,  1,  3,  3,  9,  8, 14,  5, 10, 10,  1, 18, 13,\n",
       "       13, 10, 16, 17, 18, 19,  2,  1,  0,  2,  4,  5,  6,  7,  8, 14, 15,\n",
       "       10, 15, 13, 14,  1, 19, 19, 18, 19,  0, 11, 10, 10, 11,  9,  6, 11,\n",
       "        5,  5,  1, 11, 12, 14, 12, 15, 12, 19, 18, 19,  0, 10,  0,  7, 16,\n",
       "        7,  6,  7,  8,  9, 10, 10,  6, 13, 14,  1, 16, 17, 18, 19,  0, 15,\n",
       "        0,  3, 10,  5, 16,  7,  6,  9, 10, 15, 12, 13, 11,  3, 16, 17, 18,\n",
       "       19,  0,  1,  4, 11,  4,  5,  6,  7,  5,  5,  1,  1, 12, 13, 14, 15,\n",
       "       17, 19, 18, 19,  2,  1,  1,  3,  5,  7,  6,  7,  8,  9, 10,  5, 12,\n",
       "       13, 14, 15, 16, 17, 18, 19,  0,  1,  2,  3, 11,  5,  8,  7,  8,  9,\n",
       "       10,  1, 12, 13, 14, 15, 16, 17, 18, 19,  0,  1,  2,  3,  4,  0,  6,\n",
       "        7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 11,  1,  1, 15,\n",
       "        4,  5,  6,  7,  8,  9, 10,  1, 12, 13, 14, 15, 16, 17, 18, 19,  0,\n",
       "        1,  2,  3, 11,  5,  6,  7,  8,  9, 10,  1, 12, 13, 14, 10, 16, 17,\n",
       "       18, 19,  0,  1,  2,  3,  2, 11, 12,  7,  8,  9, 10, 11, 12, 13, 14,\n",
       "       15, 16, 17, 18, 19,  0, 11,  2, 15,  4,  5,  6,  7,  8,  2, 10, 11,\n",
       "       12, 13, 14, 15, 16, 19, 18, 19], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(onehot_predict, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 10, 17, 17,  3, 14,  7,  3,  7,  3, 11, 14,  3, 14, 14, 14,  3,\n",
       "       13, 14,  3,  3, 14,  3, 14,  3, 14, 14,  3, 14, 14,  3,  3, 14, 14,\n",
       "        3,  3, 14, 14,  3,  3,  3,  6, 13, 13, 14, 14,  3,  3,  3, 14,  3,\n",
       "       14, 13, 14,  3, 14,  3, 14,  3,  3,  3,  3,  3, 14, 14, 14,  3,  7,\n",
       "        3,  3, 14, 14, 13,  3, 14, 14, 14, 14, 14,  3, 14, 14,  3, 14,  3,\n",
       "       14,  3, 14, 14,  3, 13, 14, 14,  3,  3, 14, 14, 14, 14,  3,  3,  3,\n",
       "        3, 14, 14, 14, 14, 14, 14, 13,  3,  3, 14,  3,  6, 14,  3, 16, 14,\n",
       "        3, 11, 13, 14,  7, 14,  3, 14,  3,  3, 14, 14, 14,  3,  7, 14,  3,\n",
       "        3,  3, 14, 14], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "predict_matrix = []\n",
    "val_data_loader = DataLoader(BonzDataset(test_data), batch_size=32)\n",
    "for img_tensors, features in val_data_loader:\n",
    "    img_tensors = [ts.to(DEVICE) for ts in img_tensors]\n",
    "    features = features.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        predict = model(img_tensors, features)\n",
    "    predict = predict.detach().cpu()\n",
    "    predict_matrix.extend(torch.softmax(predict, 1))\n",
    "\n",
    "onehot_predict = torch.stack(predict_matrix).numpy()\n",
    "\n",
    "COLUMN_NAMES = ['act'+str(i)+str(j) for i in range(2) for j in range(10)]\n",
    "COLUMN_NAMES.pop(0)\n",
    "COLUMN_NAMES.append('act20')\n",
    "\n",
    "df_prob = pd.DataFrame(data=onehot_predict, columns=COLUMN_NAMES)\n",
    "df_prob['event'] = test_data['image_prefixes']\n",
    "\n",
    "submission = []\n",
    "for act in list(df_prob.keys())[:-1]:\n",
    "    ranked_ = df_prob.sort_values(by=[act], ascending=False)['event'].values\n",
    "    ranked_ = act+' '+ranked_\n",
    "    submission.extend(ranked_)\n",
    "\n",
    "pd.DataFrame(submission).to_csv('submission.txt', header=False, index=False)\n",
    "\n",
    "np.argmax(onehot_predict, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
