{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms as T\n",
    "from torchvision.models.alexnet import AlexNet\n",
    "from torchvision.models.resnet import resnet18, resnet34, resnet50, resnet101, resnet152\n",
    "from torch.utils.data import IterableDataset, DataLoader, Dataset\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, average_precision_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "def linear_combination(x, y, epsilon): \n",
    "    return epsilon*x + (1-epsilon)*y\n",
    "\n",
    "\n",
    "def reduce_loss(loss, reduction='mean'):\n",
    "    return loss.mean() if reduction=='mean' else loss.sum() if reduction=='sum' else loss\n",
    "\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, epsilon:float=0.1, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, preds, target):\n",
    "        n = preds.size()[-1]\n",
    "        log_preds = F.log_softmax(preds, dim=-1)\n",
    "        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)\n",
    "        nll = F.nll_loss(log_preds, target, reduction=self.reduction)\n",
    "        return linear_combination(loss/n, nll, self.epsilon)\n",
    "    \n",
    "    \n",
    "class BonzDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super(BonzDataset, self).__init__()\n",
    "        self.image_prefixes = data.image_prefixes.values\n",
    "        self.features = data.features.values\n",
    "        self.img_tensors = data.img_tensors.values\n",
    "        if 'labels' in data:\n",
    "            self.labels = data.labels.values\n",
    "            self.one_hot_labels = data.one_hot_labels.values\n",
    "        else:\n",
    "            self.labels=None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_prefixes)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        outputs = (self.img_tensors[idx], self.features[idx],)\n",
    "        if self.labels is not None:\n",
    "            outputs += (self.one_hot_labels[idx], self.labels[idx])\n",
    "        return outputs\n",
    "    \n",
    "        \n",
    "class Bonz(nn.Module):\n",
    "    def __init__(self, hidden_dim=100, feature_selection=12):\n",
    "        super(Bonz, self).__init__()\n",
    "        self.resnet = resnet50(True)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.BiLSTM = nn.LSTM(2048, hidden_dim, num_layers=2, batch_first=True, bidirectional=True, dropout=0)\n",
    "        self.bn = nn.BatchNorm1d(hidden_dim*2 + feature_selection)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim*2 + feature_selection, hidden_dim*2 + feature_selection),\n",
    "            nn.LeakyReLU(0.001, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim*2 + feature_selection, hidden_dim*2),\n",
    "            nn.LeakyReLU(0.001, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim*2, 20)\n",
    "        )\n",
    "        #self.classifier.apply(self._init_weights)\n",
    "        self.classifier.apply(self._xavier)\n",
    "        self.BiLSTM.apply(self._xavier)\n",
    "    \n",
    "    def forward(self, imgs, features=None):\n",
    "        # Generate featuress from each images\n",
    "        x = []\n",
    "        for img in imgs:\n",
    "            temp = self.do_resnet(img)\n",
    "            x.append(temp.unsqueeze(1))\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # LSTM step\n",
    "        x, _ = self.BiLSTM(x)\n",
    "        lstm_features = x[:,-1,:]\n",
    "        \n",
    "        # Concate features\n",
    "        total_features = torch.cat([lstm_features, features], -1)\n",
    "        total_features = self.bn(total_features)\n",
    "        predict = self.classifier(total_features)\n",
    "\n",
    "        return (predict, lstm_features)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding, nn.Conv1d)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, (nn.Linear, nn.Conv1d)) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "            \n",
    "    def _xavier(self, module):\n",
    "        for name, param in module.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "            elif 'bias' in name:\n",
    "                param.data.zero_()\n",
    "    \n",
    "    def do_resnet(self, img):\n",
    "        x = self.resnet.conv1(img)\n",
    "        x = self.resnet.bn1(x)\n",
    "        x = self.resnet.relu(x)\n",
    "        x = self.resnet.maxpool(x)\n",
    "\n",
    "        x = self.resnet.layer1(x)\n",
    "        x = self.resnet.layer2(x)\n",
    "        x = self.resnet.layer3(x)\n",
    "        x = self.resnet.layer4(x)\n",
    "\n",
    "        x = self.resnet.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def freeze_resnet(self):\n",
    "        for w in self.resnet.parameters():\n",
    "            w.requires_grad = False\n",
    "            \n",
    "    def unfreeze_resnet(self):\n",
    "        for w in self.resnet.parameters():\n",
    "            w.requires_grad = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Bonz()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame({'a': [0,1,2]*5, 'b': [0,1,2]*5})\n",
    "data.a.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "699c7eadda7a43b5bdb59af909067142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=280.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3518226da2244106a6b4f55b3036712d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=140.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "59363526\n",
      "84920558\n"
     ]
    }
   ],
   "source": [
    "def get_data(path):\n",
    "    data = pd.read_csv(path)\n",
    "    resnet_idx = list(data.columns).index('data_AUTOGRAPHER_RESNET_mean_tench, Tinca tinca')\n",
    "    new_data = data.iloc[:,:resnet_idx]\n",
    "    new_data = new_data.drop([col_len for col_len in new_data.keys() if '_len' in col_len], 1) # Drop columns with _LEN\n",
    "    new_data['labels'] = [int(i[-2:])-1 for i in new_data.event_id.values]\n",
    "    new_data['image_prefixes'] = list(map(lambda x,y,z: str(x)+'_'+str(y)+'_'+str(z), \n",
    "                                      new_data.sub_id.values, \n",
    "                                      new_data.source.values, \n",
    "                                      new_data.event_id.values))\n",
    "    new_data['one_hot_labels'] = list(map(lambda x: nn.functional.one_hot(torch.tensor(x), 20).float(), new_data.labels.values))\n",
    "    new_data['features'] = [torch.tensor(i).float() for i in new_data.iloc[:, 3:-3].values]\n",
    "    new_data['img_tensors'] = get_img_tensors(new_data['image_prefixes'].values)\n",
    "    return new_data.iloc[:, -5:]\n",
    "\n",
    "\n",
    "def get_test_data(path):\n",
    "    data = pd.read_csv(path, index_col=0)\n",
    "    resnet_idx = list(data.columns).index('data_AUTOGRAPHER_RESNET_mean_tench, Tinca tinca')\n",
    "    new_data = data.iloc[:,:resnet_idx]\n",
    "    new_data = new_data.drop([col_len for col_len in new_data.keys() if '_len' in col_len], 1) # Drop columns with _LEN\n",
    "    new_data['image_prefixes'] = list(map(lambda x,y: str(x)+'_pred'+str(y), \n",
    "                                      new_data.sub_id.values, \n",
    "                                      new_data.event_id.values))\n",
    "    new_data['features'] = [torch.tensor(i).float() for i in new_data.iloc[:, 3:-1].values]\n",
    "    new_data['img_tensors'] = get_img_tensors(new_data['image_prefixes'].values)\n",
    "    return new_data.iloc[:, [0,1,2,-3,-2,-1]]\n",
    "\n",
    "\n",
    "def get_img_tensors(image_prefixes):\n",
    "    transform = T.Compose([\n",
    "        T.Resize(256),\n",
    "        T.CenterCrop(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    temp = []\n",
    "    \n",
    "    for img_prefix in tqdm.notebook.tqdm(image_prefixes):\n",
    "        img_paths = []\n",
    "        i = 0\n",
    "        img_name = img_prefix+'_'+str(i)+'.jpg'\n",
    "        while img_name in os.listdir('./OUTPUT_MERGED/AUTOGRAPHER/'):\n",
    "            img_paths.append('./OUTPUT_MERGED/AUTOGRAPHER/'+img_name)\n",
    "            i += 1\n",
    "            img_name = img_prefix+'_'+str(i)+'.jpg'\n",
    "\n",
    "        # Transform images to tensors\n",
    "        img_tensors = []\n",
    "        for path in img_paths:\n",
    "            img = Image.open(path)\n",
    "            img_tensors.append(transform(img))\n",
    "            \n",
    "        # padding img tensors\n",
    "        if len(img_tensors) < 16:\n",
    "            dump = torch.zeros((3,224,224)).float()\n",
    "            dump = [dump] * (16 - len(img_tensors))\n",
    "            img_tensors.extend(dump)\n",
    "            \n",
    "        temp.append(img_tensors)\n",
    "    \n",
    "    return temp\n",
    "\n",
    "def check_params(model):\n",
    "    model.freeze_resnet()\n",
    "    print(sum([i.numel() for i in model.parameters() if i.requires_grad]))\n",
    "    model.unfreeze_resnet()\n",
    "    print(sum([i.numel() for i in model.parameters() if i.requires_grad]))\n",
    "    \n",
    "\n",
    "        \n",
    "train_data = get_data('train_min_max.csv')\n",
    "#train_data = train_data.sort_values(by='image_prefixes', ignore_index=True)\n",
    "test_data = get_test_data('test_min_max.csv')\n",
    "\n",
    "\n",
    "model = Bonz(hidden_dim=1024, feature_selection=train_data.features[0].shape[0])\n",
    "model.to(DEVICE)\n",
    "torch.save(model.state_dict(), 'origin_sd.pt')\n",
    "origin_state_dict = torch.load('origin_sd.pt')\n",
    "\n",
    "check_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1787686347935873e-06\t3.0017285346984863\n",
      "1.3894954943731377e-06\t3.0015506744384766\n",
      "1.637893706954064e-06\t3.0012686252593994\n",
      "1.93069772888325e-06\t3.0019760131835938\n",
      "2.275845926074788e-06\t3.0035910606384277\n",
      "2.6826957952797253e-06\t3.00378680229187\n",
      "3.162277660168379e-06\t3.0037596225738525\n",
      "3.72759372031494e-06\t3.0027661323547363\n",
      "4.393970560760791e-06\t3.0021867752075195\n",
      "5.1794746792312115e-06\t3.0006489753723145\n",
      "6.1054022965853284e-06\t3.0004799365997314\n",
      "7.19685673001152e-06\t3.0001814365386963\n",
      "8.48342898244072e-06\t2.999457359313965\n",
      "1e-05\t3.0001561641693115\n",
      "1.1787686347935872e-05\t2.999746561050415\n",
      "1.3894954943731378e-05\t3.000974178314209\n",
      "1.6378937069540644e-05\t3.0009868144989014\n",
      "1.93069772888325e-05\t3.001993179321289\n",
      "2.2758459260747882e-05\t3.0037615299224854\n",
      "2.682695795279726e-05\t3.0045859813690186\n",
      "3.162277660168379e-05\t3.004239082336426\n",
      "3.72759372031494e-05\t3.004592180252075\n",
      "4.39397056076079e-05\t3.0039453506469727\n",
      "5.1794746792312105e-05\t3.00384521484375\n",
      "6.105402296585325e-05\t3.0045931339263916\n",
      "7.196856730011521e-05\t3.0046043395996094\n",
      "8.483428982440718e-05\t3.0044257640838623\n",
      "0.00010000000000000003\t3.004713535308838\n",
      "0.00011787686347935874\t3.00347638130188\n",
      "0.00013894954943731373\t3.003365993499756\n",
      "0.00016378937069540635\t3.0033204555511475\n",
      "0.00019306977288832504\t3.0031678676605225\n",
      "0.00022758459260747876\t3.003695011138916\n",
      "0.00026826957952797266\t3.004265785217285\n",
      "0.00031622776601683794\t3.0047733783721924\n",
      "0.00037275937203149395\t3.0054855346679688\n",
      "0.0004393970560760789\t3.006074905395508\n",
      "0.0005179474679231212\t3.006152868270874\n",
      "0.0006105402296585327\t3.0061044692993164\n",
      "0.0007196856730011523\t3.0064220428466797\n",
      "0.0008483428982440721\t3.0059728622436523\n",
      "0.0009999999999999998\t3.006669044494629\n",
      "0.0011787686347935866\t3.0058107376098633\n",
      "0.0013894954943731376\t3.005753517150879\n",
      "0.0016378937069540635\t3.0061447620391846\n",
      "0.001930697728883249\t3.005131959915161\n",
      "0.0022758459260747883\t3.005268096923828\n",
      "0.002682695795279725\t3.005033016204834\n",
      "0.0031622776601683803\t3.005941867828369\n",
      "0.0037275937203149366\t3.005241870880127\n",
      "0.004393970560760798\t3.004911184310913\n",
      "0.005179474679231212\t3.0055105686187744\n",
      "0.0061054022965853225\t3.0044748783111572\n",
      "0.007196856730011518\t3.0044398307800293\n",
      "0.008483428982440722\t3.0044643878936768\n",
      "0.010000000000000009\t3.0032119750976562\n",
      "0.011787686347935868\t3.0025038719177246\n",
      "0.013894954943731379\t3.003770112991333\n",
      "0.016378937069540654\t3.003833293914795\n",
      "0.019306977288832496\t3.002930164337158\n",
      "0.022758459260747887\t3.002925395965576\n",
      "0.02682695795279723\t3.0026838779449463\n",
      "0.03162277660168384\t3.0026602745056152\n",
      "0.03727593720314941\t3.0020575523376465\n",
      "0.04393970560760786\t3.0018460750579834\n",
      "0.051794746792312094\t3.001218795776367\n",
      "0.0610540229658533\t3.0012435913085938\n",
      "0.07196856730011526\t3.0021872520446777\n",
      "0.08483428982440716\t3.003098964691162\n",
      "0.1\t3.0024375915527344\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "model.freeze_resnet()\n",
    "model.load_state_dict(origin_state_dict)\n",
    "\n",
    "start_lr = 1e-6\n",
    "end_lr = 0.1\n",
    "lr_find_epochs = 10\n",
    "\n",
    "dataloader = DataLoader(BonzDataset(train_data), batch_size=40)\n",
    "\n",
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), start_lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "lr_lambda = lambda x: math.exp(x * math.log(end_lr / start_lr) / (lr_find_epochs * len(dataloader)))\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Make lists to capture the logs\n",
    "\n",
    "lr_find_loss = []\n",
    "lr_find_lr = []\n",
    "\n",
    "iter = 0\n",
    "\n",
    "smoothing = 0.05\n",
    "\n",
    "for i in range(lr_find_epochs):\n",
    "    for img_tensors, features, one_hot_label, label in dataloader:\n",
    "\n",
    "        # Send to device\n",
    "        img_tensors = [ts.to(DEVICE) for ts in img_tensors]\n",
    "        features = features.to(DEVICE)\n",
    "        one_hot_label = one_hot_label.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "\n",
    "        # Training mode and zero gradients\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get outputs to calc loss\n",
    "        outputs = model(img_tensors, features)\n",
    "        loss = criterion(outputs, label)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update LR\n",
    "        scheduler.step()\n",
    "        lr_step = optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
    "        lr_find_lr.append(lr_step)\n",
    "\n",
    "        # smooth the loss\n",
    "        if iter==0:\n",
    "            lr_find_loss.append(loss.item())\n",
    "        else:\n",
    "            loss = smoothing  * loss + (1 - smoothing) * lr_find_loss[-1]\n",
    "            lr_find_loss.append(loss.item())\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "for a, b in zip(lr_find_lr, lr_find_loss):\n",
    "    print(f'{a}\\t{b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BATCH_SIZE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-5ef909fc83f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mlr_find_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mdataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBonzDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BATCH_SIZE' is not defined"
     ]
    }
   ],
   "source": [
    "model.freeze_resnet()\n",
    "model.load_state_dict(origin_state_dict)\n",
    "\n",
    "start_lr = 1e-6\n",
    "end_lr = 0.1\n",
    "lr_find_epochs = 20\n",
    "\n",
    "dataloader = DataLoader(BonzDataset(train_data), batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), start_lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                              base_lr=start_lr, \n",
    "                                              max_lr=end_lr, \n",
    "                                              step_size_up=lr_find_epochs)\n",
    "\n",
    "# Make lists to capture the logs\n",
    "\n",
    "lr_find_acc = []\n",
    "lr_find_lr = []\n",
    "\n",
    "for i in range(lr_find_epochs):\n",
    "    \n",
    "    predicts = []\n",
    "    y_true = []\n",
    "    \n",
    "    for img_tensors, features, one_hot_label, label in dataloader:\n",
    "\n",
    "        img_tensors = [ts.to(DEVICE) for ts in img_tensors]\n",
    "        features = features.to(DEVICE)\n",
    "        one_hot_label = one_hot_label.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predict = model(img_tensors, features)\n",
    "        loss = criterion(predict, label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        predict = predict.detach().cpu()\n",
    "        predict = torch.argmax(predict, 1)\n",
    "        predicts.extend(predict.tolist())\n",
    "        y_true.extend(label.detach().cpu().tolist())\n",
    "    \n",
    "    train_acc = accuracy_score(y_true, predicts)\n",
    "    print(f'epoch={i}, Acc={train_acc}')\n",
    "    \n",
    "    lr_step = optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
    "    lr_find_lr.append(lr_step)\n",
    "    lr_find_acc.append(train_acc)\n",
    "\n",
    "    scheduler.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "for a, b in zip(lr_find_lr, lr_find_acc):\n",
    "    print(f'{a}\\t{b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3679f0ece03d46c5b7ad3fe03f0910a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=40.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=0, Train_loss=30.7, Val_loss=26.6, Train_acc=7.14, Val_acc=6.43, LR=2.54e-02, , Time=17.84\n",
      "BEST MODEL at Epoch = 0\n",
      "Epoch=1, Train_loss=28.7, Val_loss=29.7, Train_acc=15.36, Val_acc=5.00, LR=3.08e-02, , Time=17.58\n",
      "Epoch=2, Train_loss=25.8, Val_loss=22.2, Train_acc=18.93, Val_acc=19.29, LR=3.62e-02, , Time=17.63\n",
      "BEST MODEL at Epoch = 2\n",
      "Epoch=3, Train_loss=22.0, Val_loss=18.1, Train_acc=24.64, Val_acc=46.43, LR=4.15e-02, , Time=17.78\n",
      "BEST MODEL at Epoch = 3\n",
      "Epoch=4, Train_loss=19.3, Val_loss=30.0, Train_acc=31.07, Val_acc=28.57, LR=4.69e-02, , Time=18.10\n",
      "Epoch=5, Train_loss=19.2, Val_loss=36.0, Train_acc=35.36, Val_acc=24.64, LR=5.23e-02, , Time=18.15\n",
      "Epoch=6, Train_loss=17.4, Val_loss=36.0, Train_acc=38.93, Val_acc=20.00, LR=5.77e-02, , Time=18.17\n",
      "Epoch=7, Train_loss=14.9, Val_loss=77.2, Train_acc=50.71, Val_acc=6.07, LR=6.31e-02, , Time=18.31\n",
      "Epoch=8, Train_loss=13.7, Val_loss=63.5, Train_acc=52.50, Val_acc=11.07, LR=6.85e-02, , Time=18.37\n",
      "Epoch=9, Train_loss=10.7, Val_loss=73.5, Train_acc=60.00, Val_acc=20.00, LR=7.38e-02, , Time=18.45\n",
      "Epoch=10, Train_loss=13.0, Val_loss=1445.2, Train_acc=57.50, Val_acc=5.00, LR=7.92e-02, , Time=18.37\n",
      "Epoch=11, Train_loss=9.7, Val_loss=413.9, Train_acc=64.29, Val_acc=5.36, LR=8.46e-02, , Time=18.42\n",
      "Epoch=12, Train_loss=8.4, Val_loss=92.1, Train_acc=68.21, Val_acc=10.71, LR=9.00e-02, , Time=18.39\n",
      "Epoch=13, Train_loss=7.3, Val_loss=77.3, Train_acc=72.14, Val_acc=10.36, LR=8.46e-02, , Time=18.67\n",
      "Epoch=14, Train_loss=7.3, Val_loss=74.7, Train_acc=77.14, Val_acc=16.43, LR=7.92e-02, , Time=18.53\n",
      "Epoch=15, Train_loss=4.9, Val_loss=69.3, Train_acc=79.64, Val_acc=9.29, LR=7.38e-02, , Time=18.49\n",
      "Epoch=16, Train_loss=3.6, Val_loss=126.5, Train_acc=87.50, Val_acc=7.50, LR=6.85e-02, , Time=18.47\n",
      "Epoch=17, Train_loss=2.7, Val_loss=103.2, Train_acc=90.36, Val_acc=6.07, LR=6.31e-02, , Time=18.45\n",
      "Epoch=18, Train_loss=2.6, Val_loss=368.5, Train_acc=88.21, Val_acc=6.79, LR=5.77e-02, , Time=18.56\n",
      "Epoch=19, Train_loss=2.7, Val_loss=44.4, Train_acc=89.64, Val_acc=21.07, LR=5.23e-02, , Time=18.36\n",
      "Epoch=20, Train_loss=2.1, Val_loss=20.7, Train_acc=91.79, Val_acc=40.36, LR=4.69e-02, , Time=18.61\n",
      "Epoch=21, Train_loss=1.7, Val_loss=17.5, Train_acc=93.21, Val_acc=39.64, LR=4.15e-02, , Time=18.43\n",
      "BEST MODEL at Epoch = 21\n",
      "Epoch=22, Train_loss=1.1, Val_loss=10.2, Train_acc=95.00, Val_acc=62.50, LR=3.62e-02, , Time=18.33\n",
      "BEST MODEL at Epoch = 22\n",
      "Epoch=23, Train_loss=1.3, Val_loss=125.4, Train_acc=95.36, Val_acc=13.57, LR=3.08e-02, , Time=18.59\n",
      "Epoch=24, Train_loss=0.9, Val_loss=43.1, Train_acc=97.50, Val_acc=29.64, LR=2.54e-02, , Time=18.68\n",
      "Epoch=25, Train_loss=0.8, Val_loss=44.2, Train_acc=96.07, Val_acc=35.36, LR=2.00e-02, , Time=18.60\n",
      "Epoch=26, Train_loss=0.7, Val_loss=67.4, Train_acc=97.50, Val_acc=20.36, LR=2.54e-02, , Time=17.98\n",
      "Epoch=27, Train_loss=0.6, Val_loss=12.4, Train_acc=97.50, Val_acc=63.93, LR=3.08e-02, , Time=18.43\n",
      "Epoch=28, Train_loss=0.2, Val_loss=2.8, Train_acc=99.64, Val_acc=89.29, LR=3.62e-02, , Time=18.39\n",
      "BEST MODEL at Epoch = 28\n",
      "Epoch=29, Train_loss=0.2, Val_loss=2.1, Train_acc=99.64, Val_acc=92.86, LR=4.15e-02, , Time=18.39\n",
      "BEST MODEL at Epoch = 29\n",
      "Epoch=30, Train_loss=0.5, Val_loss=22.0, Train_acc=98.21, Val_acc=45.36, LR=4.69e-02, , Time=18.27\n",
      "Epoch=31, Train_loss=0.2, Val_loss=7.3, Train_acc=99.29, Val_acc=72.50, LR=5.23e-02, , Time=18.49\n",
      "Epoch=32, Train_loss=0.8, Val_loss=9.7, Train_acc=97.14, Val_acc=67.50, LR=5.77e-02, , Time=18.52\n",
      "Epoch=33, Train_loss=0.8, Val_loss=37.0, Train_acc=97.86, Val_acc=38.21, LR=6.31e-02, , Time=18.38\n",
      "Epoch=34, Train_loss=0.4, Val_loss=24.3, Train_acc=97.86, Val_acc=46.79, LR=6.85e-02, , Time=18.40\n",
      "Epoch=35, Train_loss=0.3, Val_loss=13.9, Train_acc=98.93, Val_acc=63.21, LR=7.38e-02, , Time=18.41\n",
      "Epoch=36, Train_loss=0.2, Val_loss=9.1, Train_acc=99.64, Val_acc=70.71, LR=7.92e-02, , Time=18.38\n",
      "Epoch=37, Train_loss=0.3, Val_loss=279.3, Train_acc=99.64, Val_acc=10.71, LR=8.46e-02, , Time=18.73\n",
      "Epoch=38, Train_loss=0.2, Val_loss=2.8, Train_acc=99.29, Val_acc=90.36, LR=9.00e-02, , Time=18.46\n",
      "Epoch=39, Train_loss=0.2, Val_loss=8.3, Train_acc=99.64, Val_acc=76.43, LR=8.46e-02, , Time=18.38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_LR = 0.09\n",
    "MIN_LR = 0.02\n",
    "NUM_EPOCH = 40\n",
    "BATCH_SIZE = 32\n",
    "DRY_RUN = 0\n",
    "\n",
    "\n",
    "# Split data\n",
    "'''\n",
    "train_dataset = train_data.loc[140:]\n",
    "val_dataset = train_data.loc[:139]\n",
    "'''\n",
    "#train_dataset, val_dataset = train_test_split(train_data, test_size=0.2)\n",
    "\n",
    "train_dataset, val_dataset = train_data, train_data\n",
    "\n",
    "train_dataset = BonzDataset(train_dataset)\n",
    "val_dataset = BonzDataset(val_dataset)\n",
    "\n",
    "data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "best_metrics = {'train_loss': 1e10, \n",
    "                'val_loss': 1e10, \n",
    "                'train_acc': 0, \n",
    "                'val_acc': 0}\n",
    "\n",
    "metrics = {'train_loss': [], \n",
    "           'val_loss': [], \n",
    "           'train_acc': [], \n",
    "           'val_acc': []}\n",
    "\n",
    "''' TRAINING WITH FREEZE ResNet\n",
    "'''\n",
    "model.freeze_resnet()\n",
    "model.load_state_dict(origin_state_dict) #Load origin state dict\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, betas=(0.9, 0.99))\n",
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=MAX_LR)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=10, verbose=True)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                              base_lr=MIN_LR, \n",
    "                                              max_lr=MAX_LR, \n",
    "                                              step_size_up=(NUM_EPOCH//3)*len(data_loader))\n",
    "\n",
    "for e in tqdm.notebook.trange(NUM_EPOCH):\n",
    "    \n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    s = time.time()\n",
    "    train_loss = 0\n",
    "    predicts = []\n",
    "    y_true = []\n",
    "    \n",
    "    for img_tensors, features, one_hot_label, label in data_loader:\n",
    "        img_tensors = [ts.to(DEVICE) for ts in img_tensors]\n",
    "        features = features.to(DEVICE)\n",
    "        one_hot_label = one_hot_label.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predict = model(img_tensors, features)[0]\n",
    "        loss = loss_fn(predict, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if e >= DRY_RUN:\n",
    "            scheduler.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        predict = predict.detach().cpu()\n",
    "        predict = torch.argmax(predict, 1)\n",
    "        predicts.extend(predict.tolist())\n",
    "        y_true.extend(label.detach().cpu().tolist())\n",
    "\n",
    "    train_acc = accuracy_score(y_true, predicts)\n",
    "    lr_step = optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    predicts = []\n",
    "    y_true = []\n",
    "    lstm_features = []\n",
    "    \n",
    "    for img_tensors, features, one_hot_label, label in val_data_loader:\n",
    "        img_tensors = [ts.to(DEVICE) for ts in img_tensors]\n",
    "        features = features.to(DEVICE)\n",
    "        one_hot_label = one_hot_label.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predict, lstm = model(img_tensors, features)\n",
    "            loss = loss_fn(predict, label)\n",
    "            val_loss += loss.item()\n",
    "        predict = predict.detach().cpu()\n",
    "        predict = torch.argmax(predict, 1)\n",
    "        predicts.extend(predict.tolist())\n",
    "        y_true.extend(label.detach().cpu().tolist())\n",
    "        lstm_features.extend(lstm.detach().cpu().tolist())\n",
    "\n",
    "    val_acc = accuracy_score(y_true, predicts)\n",
    "\n",
    "    # Scheduler step wise\n",
    "    #scheduler.step(0.25*train_loss + 0.75*val_loss)\n",
    "    \n",
    "    print(f'Epoch={e}, Train_loss={train_loss:.1f}, Val_loss={val_loss:.1f}, Train_acc={train_acc*100:.2f}, Val_acc={val_acc*100:.2f}, LR={lr_step:.2e}, , Time={time.time()-s:.2f}')\n",
    "    \n",
    "    # Store metrics\n",
    "    metrics['train_acc'].append(train_acc)\n",
    "    metrics['val_acc'].append(val_acc)\n",
    "    metrics['train_loss'].append(train_loss)\n",
    "    metrics['val_loss'].append(val_loss)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_metrics['val_loss']:\n",
    "        best_metrics['train_acc'] = train_acc\n",
    "        best_metrics['val_acc'] = val_acc\n",
    "        best_metrics['train_loss'] = train_loss\n",
    "        best_metrics['val_loss'] = val_loss\n",
    "\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "        print(f'BEST MODEL at Epoch = {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89b6db655ed45a49dfb5f19b9bf223b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='K-Fold', max=2.0, style=ProgressStyle(description_width='â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b510127fa44743bba9764526ccce8a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST MODEL at Epoch = 0: Train_loss=27.9, Val_loss=27.0, Train_acc=0.04, Val_acc=0.05, Time=19.37\n",
      "BEST MODEL at Epoch = 2: Train_loss=27.5, Val_loss=26.6, Train_acc=0.05, Val_acc=0.05, Time=19.57\n",
      "BEST MODEL at Epoch = 3: Train_loss=27.5, Val_loss=26.0, Train_acc=0.04, Val_acc=0.08, Time=19.52\n",
      "BEST MODEL at Epoch = 4: Train_loss=25.5, Val_loss=23.1, Train_acc=0.09, Val_acc=0.14, Time=19.57\n",
      "BEST MODEL at Epoch = 8: Train_loss=23.9, Val_loss=22.3, Train_acc=0.11, Val_acc=0.21, Time=20.09\n",
      "BEST MODEL at Epoch = 10: Train_loss=21.9, Val_loss=20.2, Train_acc=0.12, Val_acc=0.23, Time=20.01\n",
      "BEST MODEL at Epoch = 11: Train_loss=20.0, Val_loss=18.6, Train_acc=0.20, Val_acc=0.29, Time=20.05\n",
      "BEST MODEL at Epoch = 13: Train_loss=18.9, Val_loss=17.1, Train_acc=0.26, Val_acc=0.30, Time=20.07\n",
      "BEST MODEL at Epoch = 14: Train_loss=18.3, Val_loss=15.6, Train_acc=0.24, Val_acc=0.36, Time=20.25\n",
      "BEST MODEL at Epoch = 18: Train_loss=17.2, Val_loss=14.7, Train_acc=0.32, Val_acc=0.39, Time=19.98\n",
      "BEST MODEL at Epoch = 19: Train_loss=15.9, Val_loss=13.1, Train_acc=0.36, Val_acc=0.44, Time=20.26\n",
      "BEST MODEL at Epoch = 24: Train_loss=14.7, Val_loss=12.7, Train_acc=0.38, Val_acc=0.48, Time=19.90\n",
      "BEST MODEL at Epoch = 26: Train_loss=15.8, Val_loss=11.6, Train_acc=0.41, Val_acc=0.54, Time=19.92\n",
      "BEST MODEL at Epoch = 29: Train_loss=12.5, Val_loss=9.5, Train_acc=0.47, Val_acc=0.60, Time=20.00\n",
      "Epoch    36: reducing learning rate of group 0 to 5.0000e-02.\n",
      "BEST MODEL at Epoch = 36: Train_loss=11.4, Val_loss=8.6, Train_acc=0.55, Val_acc=0.67, Time=20.10\n",
      "BEST MODEL at Epoch = 38: Train_loss=10.6, Val_loss=8.0, Train_acc=0.55, Val_acc=0.69, Time=20.02\n",
      "BEST MODEL at Epoch = 39: Train_loss=10.1, Val_loss=7.5, Train_acc=0.55, Val_acc=0.72, Time=19.73\n",
      "BEST MODEL at Epoch = 41: Train_loss=9.9, Val_loss=7.0, Train_acc=0.57, Val_acc=0.72, Time=19.88\n",
      "BEST MODEL at Epoch = 43: Train_loss=9.6, Val_loss=6.7, Train_acc=0.61, Val_acc=0.74, Time=19.79\n",
      "BEST MODEL at Epoch = 49: Train_loss=9.7, Val_loss=6.3, Train_acc=0.56, Val_acc=0.75, Time=20.17\n",
      "BEST MODEL at Epoch = 53: Train_loss=8.5, Val_loss=6.3, Train_acc=0.63, Val_acc=0.77, Time=19.89\n",
      "Epoch    60: reducing learning rate of group 0 to 2.5000e-02.\n",
      "BEST MODEL at Epoch = 60: Train_loss=8.7, Val_loss=5.9, Train_acc=0.66, Val_acc=0.81, Time=20.08\n",
      "BEST MODEL at Epoch = 61: Train_loss=8.7, Val_loss=5.3, Train_acc=0.64, Val_acc=0.82, Time=20.22\n",
      "BEST MODEL at Epoch = 65: Train_loss=7.8, Val_loss=4.5, Train_acc=0.68, Val_acc=0.86, Time=19.99\n",
      "Epoch    72: reducing learning rate of group 0 to 1.2500e-02.\n",
      "BEST MODEL at Epoch = 72: Train_loss=6.2, Val_loss=4.3, Train_acc=0.77, Val_acc=0.87, Time=19.97\n",
      "BEST MODEL at Epoch = 74: Train_loss=6.0, Val_loss=4.0, Train_acc=0.78, Val_acc=0.87, Time=20.11\n",
      "BEST MODEL at Epoch = 75: Train_loss=5.5, Val_loss=4.0, Train_acc=0.78, Val_acc=0.88, Time=20.12\n",
      "BEST MODEL at Epoch = 77: Train_loss=5.9, Val_loss=3.8, Train_acc=0.77, Val_acc=0.89, Time=20.04\n",
      "Epoch    84: reducing learning rate of group 0 to 6.2500e-03.\n",
      "BEST MODEL at Epoch = 86: Train_loss=5.3, Val_loss=3.6, Train_acc=0.79, Val_acc=0.89, Time=20.04\n",
      "BEST MODEL at Epoch = 88: Train_loss=5.7, Val_loss=3.6, Train_acc=0.80, Val_acc=0.90, Time=19.92\n",
      "BEST MODEL at Epoch = 93: Train_loss=5.4, Val_loss=3.5, Train_acc=0.80, Val_acc=0.90, Time=19.91\n",
      "BEST MODEL at Epoch = 96: Train_loss=6.0, Val_loss=3.4, Train_acc=0.78, Val_acc=0.91, Time=20.11\n",
      "BEST MODEL at Epoch = 103: Train_loss=5.1, Val_loss=3.4, Train_acc=0.81, Val_acc=0.91, Time=20.45\n",
      "BEST MODEL at Epoch = 105: Train_loss=5.7, Val_loss=3.3, Train_acc=0.76, Val_acc=0.91, Time=20.12\n",
      "BEST MODEL at Epoch = 115: Train_loss=5.4, Val_loss=3.2, Train_acc=0.79, Val_acc=0.92, Time=20.32\n",
      "BEST MODEL at Epoch = 116: Train_loss=5.2, Val_loss=3.2, Train_acc=0.80, Val_acc=0.92, Time=19.81\n",
      "Epoch   123: reducing learning rate of group 0 to 3.1250e-03.\n",
      "BEST MODEL at Epoch = 125: Train_loss=4.8, Val_loss=3.1, Train_acc=0.84, Val_acc=0.93, Time=20.00\n",
      "BEST MODEL at Epoch = 135: Train_loss=5.1, Val_loss=3.0, Train_acc=0.81, Val_acc=0.93, Time=20.02\n",
      "Epoch   145: reducing learning rate of group 0 to 1.5625e-03.\n",
      "BEST MODEL at Epoch = 149: Train_loss=5.0, Val_loss=2.9, Train_acc=0.82, Val_acc=0.94, Time=19.94\n",
      "Epoch   157: reducing learning rate of group 0 to 7.8125e-04.\n",
      "Epoch   163: reducing learning rate of group 0 to 3.9063e-04.\n",
      "BEST MODEL at Epoch = 165: Train_loss=4.7, Val_loss=2.9, Train_acc=0.83, Val_acc=0.94, Time=20.13\n",
      "Epoch   176: reducing learning rate of group 0 to 1.9531e-04.\n",
      "Epoch   182: reducing learning rate of group 0 to 9.7656e-05.\n",
      "Epoch   188: reducing learning rate of group 0 to 4.8828e-05.\n",
      "Epoch   194: reducing learning rate of group 0 to 2.4414e-05.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13aef4a5685f4456858ed19e5284fd33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    10: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    21: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch    27: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch    34: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch    40: reducing learning rate of group 0 to 3.1250e-06.\n",
      "Epoch    46: reducing learning rate of group 0 to 1.5625e-06.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' Training with UNFREEZE ResNet\n",
    "''' \n",
    "model.unfreeze_resnet()\n",
    "#optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, betas=(0.9, 0.99))\n",
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4, momentum=0.9, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "for e in tqdm.notebook.trange(50):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    s = time.time()\n",
    "    train_loss = 0\n",
    "    predicts = []\n",
    "    y_true = []\n",
    "    data_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    for img_tensors, features, one_hot_label, label in data_loader:\n",
    "        img_tensors = [ts.to(DEVICE) for ts in img_tensors]\n",
    "        features = features.to(DEVICE)\n",
    "        one_hot_label = one_hot_label.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predict = model(img_tensors, features)\n",
    "        loss = loss_fn(predict, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        predict = predict.detach().cpu()\n",
    "        predict = torch.argmax(predict, 1)\n",
    "        predicts.extend(predict.tolist())\n",
    "        y_true.extend(label.detach().cpu().tolist())\n",
    "\n",
    "    train_acc = accuracy_score(y_true, predicts)\n",
    "\n",
    "    # Validate data\n",
    "    model.eval()\n",
    "    predicts = []\n",
    "    y_true = []\n",
    "    val_loss = 0\n",
    "    val_data_loader = DataLoader(val_dataset, batch_size=4)\n",
    "    for img_tensors, features, one_hot_label, label in val_data_loader:\n",
    "        img_tensors = [ts.to(DEVICE) for ts in img_tensors]\n",
    "        features = features.to(DEVICE)\n",
    "        one_hot_label = one_hot_label.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predict = model(img_tensors, features)\n",
    "            loss = loss_fn(predict, label)\n",
    "            val_loss += loss.item()\n",
    "        predict = predict.detach().cpu()\n",
    "        predict = torch.argmax(predict, 1)\n",
    "        predicts.extend(predict.tolist())\n",
    "        y_true.extend(label.detach().cpu().tolist())\n",
    "\n",
    "    val_acc = accuracy_score(y_true, predicts)\n",
    "\n",
    "    # Scheduler step wise\n",
    "    scheduler.step(0.1*train_loss + 0.9*val_loss)\n",
    "\n",
    "    if (0.1*train_loss + 0.9*val_loss) <= (0.1*best_metrics['train_loss'] + 0.9*best_metrics['val_loss']):\n",
    "        if (0.1*train_acc + 0.9*val_acc) > (0.1*best_metrics['train_acc'] + 0.9*best_metrics['val_acc']):\n",
    "            best_metrics['train_acc'] = train_acc\n",
    "            best_metrics['val_acc'] = val_acc\n",
    "            best_metrics['train_loss'] = train_loss\n",
    "            best_metrics['val_loss'] = val_loss\n",
    "\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "            print(f'BEST MODEL at Epoch = {e}: Train_loss={train_loss:.1f}, Val_loss={val_loss:.1f}, Train_acc={train_acc:.2f}, Val_acc={val_acc:.2f}, Time={time.time()-s:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9951127895797012"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "predict_matrix = []\n",
    "lstm_features = []\n",
    "val_loss = 0\n",
    "val_data_loader = DataLoader(BonzDataset(train_data), batch_size=BATCH_SIZE)\n",
    "\n",
    "for img_tensors, features, one_hot_label, label in val_data_loader:\n",
    "    img_tensors = [ts.to(DEVICE) for ts in img_tensors]\n",
    "    features = features.to(DEVICE)\n",
    "    one_hot_label = one_hot_label.to(DEVICE)\n",
    "    label = label.to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predict, lstm = model(img_tensors, features)\n",
    "        \n",
    "    predict = predict.detach().cpu()\n",
    "    predict_matrix.extend(torch.softmax(predict, 1))\n",
    "    lstm_features.extend(lstm.detach().cpu().tolist())\n",
    "    \n",
    "onehot_true = np.array(list(i.numpy() for i in train_data.one_hot_labels.values))\n",
    "onehot_predict = torch.stack(predict_matrix).numpy()\n",
    "\n",
    "average_precision_score(onehot_true, onehot_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.DataFrame(lstm_features)\n",
    "train_features.to_csv('train_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7,  0,  7, 14,  7, 18, 18, 11,  7,  7, 13, 12, 14, 13, 12, 14, 11,\n",
       "       13, 14, 14, 13, 12, 13, 14, 13, 13, 13, 11, 12, 13, 13, 13,  7,  7,\n",
       "       13, 12, 14, 18, 13, 13, 13, 18, 13, 13, 14, 12, 13, 13, 11, 13, 14,\n",
       "       14, 13, 13, 13, 13,  7, 13, 13, 11,  7, 13, 14, 13, 13, 12, 13, 18,\n",
       "       11, 12, 13, 13, 13, 13, 14,  7, 13, 12,  7, 13, 12, 12, 11, 19, 11,\n",
       "       12, 13, 11,  7, 13, 13, 18, 13, 11, 12, 12, 13, 14,  7,  7, 13, 12,\n",
       "       13,  7, 12, 12, 13, 13, 12, 13, 13, 13, 13, 13, 18,  7, 13, 16, 13,\n",
       "       13, 11, 13, 14,  7, 18, 12, 14, 13, 13, 13, 13,  7, 13,  7, 12, 13,\n",
       "       13, 13, 12, 13], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "predict_matrix = []\n",
    "lstm_features = []\n",
    "val_data_loader = DataLoader(BonzDataset(test_data), batch_size=BATCH_SIZE)\n",
    "\n",
    "for img_tensors, features in val_data_loader:\n",
    "    img_tensors = [ts.to(DEVICE) for ts in img_tensors]\n",
    "    features = features.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        predict, lstm = model(img_tensors, features)\n",
    "    predict = predict.detach().cpu()\n",
    "    predict_matrix.extend(torch.softmax(predict, 1))\n",
    "    lstm_features.extend(lstm.detach().cpu().tolist())\n",
    "    \n",
    "test_features = pd.DataFrame(lstm_features)\n",
    "test_features.to_csv('test_features.csv', index=False)\n",
    "\n",
    "onehot_predict = torch.stack(predict_matrix).numpy()\n",
    "\n",
    "COLUMN_NAMES = ['act'+str(i)+str(j) for i in range(2) for j in range(10)]\n",
    "COLUMN_NAMES.pop(0)\n",
    "COLUMN_NAMES.append('act20')\n",
    "\n",
    "df_prob = pd.DataFrame(data=onehot_predict, columns=COLUMN_NAMES)\n",
    "df_prob['event'] = test_data['image_prefixes']\n",
    "\n",
    "submission = []\n",
    "for act in list(df_prob.keys())[:-1]:\n",
    "    ranked_ = df_prob.sort_values(by=[act], ascending=False)['event'].values\n",
    "    ranked_ = act+' '+ranked_\n",
    "    submission.extend(ranked_)\n",
    "\n",
    "pd.DataFrame(submission).to_csv('submission.txt', header=False, index=False)\n",
    "\n",
    "np.argmax(onehot_predict, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
